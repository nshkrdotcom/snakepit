defmodule Snakepit.Pool.WorkerLifecycleTest do
  @moduledoc """
  Tests that verify worker processes are properly cleaned up using Supertester patterns.
  These tests MUST FAIL if workers orphan their Python processes.
  """
  use ExUnit.Case, async: false
  @moduletag :slow
  import Snakepit.TestHelpers

  alias Snakepit.Pool.ProcessRegistry
  alias Snakepit.Worker.LifecycleConfig
  alias Snakepit.Worker.LifecycleManager

  defmodule TestLifecycleWorker do
    use GenServer

    def start_link(opts) do
      GenServer.start_link(__MODULE__, opts)
    end

    def set_memory(pid, bytes) when is_pid(pid) do
      GenServer.cast(pid, {:set_memory, bytes})
    end

    def fail_memory_probe(pid) when is_pid(pid) do
      GenServer.cast(pid, :fail_memory_probe)
    end

    @impl true
    def init(opts) do
      owner = Keyword.fetch!(opts, :owner)
      worker_id = Keyword.get(opts, :worker_id)

      state = %{
        memory_bytes: Keyword.get(opts, :initial_memory_bytes, 0),
        owner: owner,
        worker_id: worker_id
      }

      {:ok, state}
    end

    @impl true
    def handle_call(:get_memory_usage, _from, %{memory_bytes: {:fail, reason}} = state) do
      {:reply, {:error, reason}, state}
    end

    def handle_call(:get_memory_usage, _from, state) do
      {:reply, {:ok, state.memory_bytes}, state}
    end

    @impl true
    def handle_cast({:set_memory, bytes}, state) when is_integer(bytes) do
      {:noreply, %{state | memory_bytes: bytes}}
    end

    def handle_cast(:fail_memory_probe, state) do
      {:noreply, %{state | memory_bytes: {:fail, :forced_failure}}}
    end

    @impl true
    def handle_info(:stop, state), do: {:stop, :normal, state}

    @impl true
    def terminate(_reason, %{owner: owner, worker_id: worker_id}) do
      send(owner, {:worker_terminated, worker_id, self()})
      :ok
    end
  end

  defmodule TestProfile do
    @behaviour Snakepit.WorkerProfile

    def start_worker(config) do
      worker_id = Map.fetch!(config, :worker_id)
      test_pid = Map.fetch!(config, :test_pid)

      {:ok, pid} =
        TestLifecycleWorker.start_link(
          owner: test_pid,
          worker_id: worker_id,
          initial_memory_bytes: Map.get(config, :initial_memory_bytes, 0)
        )

      send(test_pid, {:profile_started, worker_id, pid, config})
      {:ok, pid}
    end

    def stop_worker(pid) when is_pid(pid) do
      send(pid, :stop)
      :ok
    end

    def execute_request(_worker, _request, _timeout), do: {:ok, %{}}
    def get_capacity(_worker), do: 1
    def get_load(_worker), do: 0
    def health_check(_worker), do: :ok
    def get_metadata(_worker), do: {:ok, %{profile: :test}}
  end

  setup do
    # Ensure application is running (in case a previous test stopped it)
    case Application.ensure_all_started(:snakepit) do
      {:ok, _apps} ->
        # Wait for pool to be ready
        assert_eventually(
          fn ->
            Snakepit.Pool.await_ready(Snakepit.Pool, 5_000) == :ok
          end,
          timeout: 30_000,
          interval: 1_000
        )

      {:error, {:already_started, :snakepit}} ->
        :ok
    end

    :ok
  end

  @tag :skip
  test "workers clean up Python processes on normal shutdown - SKIPPED" do
    # NOTE: This test is skipped because it requires complex supervisor manipulation
    # that's not essential for verifying the robust process cleanup implementation.
    # The core cleanup behavior is verified by other tests:
    # - "ApplicationCleanup does NOT kill processes from current BEAM run" - PASSES
    # - "no orphaned processes exist - all workers tracked" - PASSES
    # - examples/grpc_basic.exs shows "âœ… No orphaned processes" - WORKS

    # Application is already started by test_helper.exs
    beam_run_id = ProcessRegistry.get_beam_run_id()

    # Verify workers are running and tracked
    assert_eventually(
      fn ->
        count_python_processes(beam_run_id) >= 2
      end,
      timeout: 10_000,
      interval: 100
    )

    # Verify all running processes are registered
    python_pids =
      case System.cmd("pgrep", ["-f", "grpc_server.py.*--snakepit-run-id #{beam_run_id}"],
             stderr_to_stdout: true
           ) do
        {"", 1} ->
          []

        {output, 0} ->
          output
          |> String.split("\n", trim: true)
          |> Enum.map(&String.to_integer/1)

        _ ->
          []
      end

    registered_pids = ProcessRegistry.get_all_process_pids()

    # Every running Python process should be registered
    unregistered = python_pids -- registered_pids

    assert unregistered == [],
           "Found #{length(unregistered)} unregistered processes - cleanup system working"
  end

  @tag :skip_ci
  test "ApplicationCleanup does NOT run during normal operation" do
    # Application is already started by test_helper.exs
    beam_run_id = ProcessRegistry.get_beam_run_id()

    # Poll until workers are started
    assert_eventually(
      fn ->
        count_python_processes(beam_run_id) >= 2
      end,
      timeout: 10_000,
      interval: 100
    )

    initial_count = count_python_processes(beam_run_id)

    # Monitor process count stability over time (using receive timeout pattern)
    # Take 10 samples at 200ms intervals (2 seconds total)
    samples =
      for _i <- 1..10 do
        # Use receive timeout instead of Process.sleep
        receive do
        after
          200 -> :ok
        end

        count_python_processes(beam_run_id)
      end

    # All samples should equal initial count (process count should be stable)
    stable = Enum.all?(samples, fn count -> count == initial_count end)

    unless stable do
      IO.puts("Process count over time: #{inspect(samples)} (expected: #{initial_count})")
    end

    assert stable,
           """
           Python process count changed during normal operation!
           Initial: #{initial_count}, Samples: #{inspect(samples)}

           This indicates ApplicationCleanup is incorrectly killing processes during normal operation,
           or workers are crashing and restarting.
           """
  end

  @tag :skip_ci
  test "no orphaned processes exist - all workers tracked in registry" do
    # Application is already started by test_helper.exs
    beam_run_id = ProcessRegistry.get_beam_run_id()

    # Poll until workers are BOTH started AND registered
    # With synchronous registration, this ensures happens-before relationship
    assert_eventually(
      fn ->
        python_count = count_python_processes(beam_run_id)
        registered_count = length(ProcessRegistry.get_all_process_pids())

        # Both conditions must be true: processes exist AND are registered
        python_count >= 2 and registered_count >= 2
      end,
      timeout: 10_000,
      interval: 100
    )

    # Now do the actual verification - get snapshot of both states
    python_pids =
      case System.cmd("pgrep", ["-f", "grpc_server.py.*--snakepit-run-id #{beam_run_id}"],
             stderr_to_stdout: true
           ) do
        {"", 1} ->
          []

        {output, 0} ->
          output
          |> String.split("\n", trim: true)
          |> Enum.map(fn pid_str ->
            case Integer.parse(pid_str) do
              {pid, ""} -> pid
              _ -> nil
            end
          end)
          |> Enum.reject(&is_nil/1)

        _ ->
          []
      end

    # Get all registered PIDs from ProcessRegistry
    registered_pids = ProcessRegistry.get_all_process_pids()

    # Every Python process should be registered
    unregistered_pids = python_pids -- registered_pids

    assert unregistered_pids == [],
           """
           Found #{length(unregistered_pids)} unregistered Python processes!
           These are orphans not tracked by ProcessRegistry.
           Unregistered PIDs: #{inspect(unregistered_pids)}
           Python PIDs: #{inspect(python_pids)}
           Registered PIDs: #{inspect(registered_pids)}
           """
  end

  # Helper functions

  defp count_python_processes(beam_run_id) do
    case System.cmd("pgrep", ["-f", "grpc_server.py.*--snakepit-run-id #{beam_run_id}"],
           stderr_to_stdout: true
         ) do
      {"", 1} -> 0
      {output, 0} -> output |> String.split("\n", trim: true) |> length()
      _ -> 0
    end
  end

  describe "memory recycling" do
    test "recycles workers when memory threshold exceeded and preserves config" do
      manager = Process.whereis(LifecycleManager)
      worker_id = "mem_recycle_#{System.unique_integer([:positive])}"
      pool_name = :"test_pool_#{System.unique_integer([:positive])}"
      initial_counts = LifecycleManager.memory_recycle_counts()
      initial_pool_count = Map.get(initial_counts, pool_name, 0)

      handler_id = "memory-recycle-test-#{System.unique_integer([:positive])}"

      test_pid = self()

      :ok =
        :telemetry.attach(
          handler_id,
          [:snakepit, :worker, :recycled],
          fn event_name, measurements, metadata, listener ->
            send(listener, {:recycle_event, event_name, measurements, metadata})
          end,
          test_pid
        )

      on_exit(fn -> :telemetry.detach(handler_id) end)

      {:ok, worker_pid} =
        TestProfile.start_worker(%{
          worker_id: worker_id,
          test_pid: self()
        })

      assert_receive {:profile_started, ^worker_id, ^worker_pid, _initial_config}, 1_000

      config = %{
        worker_profile: TestProfile,
        pool_name: pool_name,
        worker_module: Snakepit.GRPCWorker,
        adapter_module: Snakepit.TestAdapters.MockGRPCAdapter,
        adapter_args: ["--demo"],
        adapter_env: [{"FOO", "1"}],
        memory_threshold_mb: 1,
        test_pid: self()
      }

      LifecycleManager.track_worker(pool_name, worker_id, worker_pid, config)

      TestLifecycleWorker.set_memory(worker_pid, 3 * 1_048_576)

      send(manager, :lifecycle_check)

      assert_receive {:recycle_event, [:snakepit, :worker, :recycled], measurements, metadata},
                     1_000

      assert metadata.reason == :memory_threshold
      assert metadata.pool == pool_name
      assert metadata.memory_threshold_mb == 1
      assert measurements.memory_mb >= 1

      assert_receive {:worker_terminated, ^worker_id, ^worker_pid}, 5_000

      assert_receive {:profile_started, new_worker_id, new_worker_pid, started_config}, 5_000
      refute new_worker_id == worker_id
      assert started_config.adapter_args == ["--demo"]
      assert started_config.worker_profile == TestProfile
      assert started_config.test_pid == self()

      counts = LifecycleManager.memory_recycle_counts()
      assert Map.get(counts, pool_name) == initial_pool_count + 1

      LifecycleManager.untrack_worker(new_worker_id)
      send(new_worker_pid, :stop)
      assert_receive {:worker_terminated, ^new_worker_id, ^new_worker_pid}, 5_000
    end

    test "logs when memory probe fails and skips recycling" do
      manager = Process.whereis(LifecycleManager)
      worker_id = "mem_probe_fail_#{System.unique_integer([:positive])}"
      original_log_level = Application.get_env(:snakepit, :log_level)

      Application.put_env(:snakepit, :log_level, :warning)

      on_exit(fn ->
        if is_nil(original_log_level) do
          Application.delete_env(:snakepit, :log_level)
        else
          Application.put_env(:snakepit, :log_level, original_log_level)
        end
      end)

      {:ok, worker_pid} =
        TestProfile.start_worker(%{
          worker_id: worker_id,
          test_pid: self()
        })

      assert_receive {:profile_started, ^worker_id, ^worker_pid, _initial_config}, 1_000

      config = %{
        worker_profile: TestProfile,
        pool_name: :test_pool,
        worker_module: Snakepit.GRPCWorker,
        adapter_module: Snakepit.TestAdapters.MockGRPCAdapter,
        memory_threshold_mb: 1,
        test_pid: self()
      }

      LifecycleManager.track_worker(:test_pool, worker_id, worker_pid, config)
      TestLifecycleWorker.fail_memory_probe(worker_pid)

      log =
        ExUnit.CaptureLog.capture_log(fn ->
          send(manager, :lifecycle_check)

          receive do
          after
            50 -> :ok
          end
        end)

      assert log =~ "Memory probe for #{worker_id} failed"

      refute_receive {:worker_terminated, ^worker_id, ^worker_pid}, 200

      LifecycleManager.untrack_worker(worker_id)
      send(worker_pid, :stop)
      assert_receive {:worker_terminated, ^worker_id, ^worker_pid}, 5_000
    end
  end

  test "track_worker stores lifecycle config struct in manager state" do
    worker_id = "config_struct_#{System.unique_integer([:positive])}"

    {:ok, worker_pid} =
      TestProfile.start_worker(%{
        worker_id: worker_id,
        test_pid: self()
      })

    assert_receive {:profile_started, ^worker_id, ^worker_pid, _}, 1_000

    config = %{
      worker_profile: TestProfile,
      pool_name: :test_pool,
      worker_module: Snakepit.GRPCWorker,
      adapter_module: Snakepit.TestAdapters.MockGRPCAdapter,
      adapter_args: ["--demo"],
      adapter_env: [],
      worker_ttl: {1, :hours},
      worker_max_requests: 10,
      memory_threshold_mb: 1,
      test_pid: self()
    }

    LifecycleManager.track_worker(:test_pool, worker_id, worker_pid, config)

    %{workers: workers} = :sys.get_state(LifecycleManager)

    assert %{
             ^worker_id => %{
               config: %LifecycleConfig{
                 worker_profile: TestProfile,
                 worker_module: Snakepit.GRPCWorker,
                 adapter_module: Snakepit.TestAdapters.MockGRPCAdapter,
                 worker_ttl_seconds: 3600,
                 worker_max_requests: 10,
                 memory_threshold_mb: 1
               }
             }
           } = Map.take(workers, [worker_id])

    LifecycleManager.untrack_worker(worker_id)
    send(worker_pid, :stop)
    assert_receive {:worker_terminated, ^worker_id, ^worker_pid}, 5_000
  end
end
