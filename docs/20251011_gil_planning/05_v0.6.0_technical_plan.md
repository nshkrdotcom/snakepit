# Snakepit v0.6.0 Technical Plan: Dual-Mode Parallelism Architecture

**Status**: Planning
**Version**: v0.6.0
**Date**: 2025-10-11
**Author**: Technical Planning Committee (synthesized from research docs 01-04)

---

## Executive Summary

Snakepit v0.6.0 introduces a **dual-mode parallelism architecture** that positions the library for the next decade of Python/Elixir integration. This release builds on v0.5.1's robust multi-process foundation by adding an optional **multi-threaded worker mode** optimized for Python 3.13+ free-threading and CPU-intensive workloads.

### Key Goals
1. **Maintain backward compatibility** - Existing users see zero breaking changes
2. **Enable future workloads** - Support Python 3.13+ free-threading for massive performance gains
3. **Give users control** - Explicit configuration for parallelism tradeoffs
4. **Provide robust defaults** - Safe, production-ready settings out of the box

---

## Table of Contents

1. [Background & Motivation](#1-background--motivation)
2. [Architecture Overview](#2-architecture-overview)
3. [Technical Design](#3-technical-design)
4. [Implementation Roadmap](#4-implementation-roadmap)
5. [Configuration API](#5-configuration-api)
6. [Python Worker Implementations](#6-python-worker-implementations)
7. [Elixir Pool Enhancements](#7-elixir-pool-enhancements)
8. [Advanced Features](#8-advanced-features)
9. [Testing Strategy](#9-testing-strategy)
10. [Documentation Plan](#10-documentation-plan)
11. [Migration Guide](#11-migration-guide)
12. [Performance Benchmarks](#12-performance-benchmarks)
13. [Risk Mitigation](#13-risk-mitigation)
14. [Future Directions](#14-future-directions)

---

## 1. Background & Motivation

### The Fundamental Problem

Snakepit bridges two powerful but philosophically different concurrency models:

- **BEAM (Erlang/Elixir)**: "Shared nothing" concurrency with millions of lightweight processes
- **Scientific Python**: "Shared memory" parallelism with heavyweight OS threads

The v0.5.1 release solved the "thread explosion" problem by enforcing single-threaded Python workers (`OPENBLAS_NUM_THREADS=1`). This is perfect for:
- High-concurrency I/O workloads (250+ workers)
- Legacy Python (≤3.12 with GIL)
- Maximum process isolation and stability

However, Python 3.13+ introduces **free-threading mode** (PEP 703), removing the Global Interpreter Lock (GIL). This unlocks a new parallelism model:
- **Multi-threaded workers** that can fully utilize multiple CPU cores
- **Shared memory** for zero-copy data transfer within a worker
- **Reduced memory overhead** (one interpreter vs many)

### Why a Dual-Mode Approach?

| Use Case | Optimal Mode | Why |
|----------|--------------|-----|
| API servers handling 1000s of small ML inference requests | **Multi-Process** | Low latency, process isolation, high concurrency |
| Data pipeline processing large tensors/dataframes | **Multi-Threaded** | Shared memory, no serialization overhead, CPU utilization |
| Training a single large neural network | **Multi-Threaded** | Full CPU core usage, shared model state |
| Legacy Python ≤3.12 integration | **Multi-Process** | GIL compatibility, proven stability |
| Mixed workloads (web + background jobs) | **Hybrid Pools** | Dedicated pools with different profiles |

---

## 2. Architecture Overview

### The Two Worker Profiles

```
┌─────────────────────────────────────────────────────────────────────┐
│                         Snakepit v0.6.0                             │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  ┌───────────────────────────────┐  ┌──────────────────────────────┐ │
│  │  Profile: :process (Default)  │  │  Profile: :thread (New)     │ │
│  ├───────────────────────────────┤  ├──────────────────────────────┤ │
│  │ • Many Python processes       │  │ • Few Python processes       │ │
│  │ • Single-threaded workers     │  │ • Multi-threaded workers     │ │
│  │ • Process isolation           │  │ • Shared memory              │ │
│  │ • OPENBLAS_NUM_THREADS=1      │  │ • Thread pool executor       │ │
│  │ • For: I/O, legacy, stability │  │ • For: CPU, Python 3.13+     │ │
│  └───────────────────────────────┘  └──────────────────────────────┘ │
│                                                                     │
│  Pool Manager: Routes to appropriate worker profile                │
│  Registry: Tracks both process-based and thread-based workers      │
│  Telemetry: Reports metrics for each profile type                  │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

### Comparison Matrix

| Feature | Multi-Process (`:process`) | Multi-Threaded (`:thread`) |
|---------|---------------------------|---------------------------|
| **Analogy** | Fleet of 200 delivery trucks 🚚 | Large warehouse with 200 workers 🏭 |
| **Processes** | Many (100-250) | Few (4-16) |
| **Threads per Process** | 1 | 8-64 |
| **Best For** | I/O-bound, high concurrency | CPU-bound, large data |
| **Memory** | High (N × interpreter) | Low (shared interpreter) |
| **Startup Time** | Slow (fork overhead) | Fast (thread spawn) |
| **Isolation** | ✅ Full process isolation | ❌ Shared process state |
| **GIL Compatibility** | ✅ Works with all Python | ⚠️ Requires Python 3.13+ |
| **Thread Safety** | ✅ Implicit | ❌ User code must be thread-safe |
| **Data Transfer** | Serialization required | Zero-copy (shared memory) |
| **Failure Blast Radius** | One worker | Entire process |

---

## 3. Technical Design

### 3.1 Configuration Schema

```elixir
# config/config.exs

config :snakepit,
  # Global defaults
  default_profile: :process,
  python_version_detection: :auto,  # Auto-detect Python version

  # Define multiple named pools
  pools: [
    # High-concurrency API pool (existing pattern)
    %{
      name: :api_pool,
      worker_profile: :process,
      pool_size: 250,
      adapter_module: Snakepit.Adapters.GRPCPython,
      adapter_env: [
        {"OPENBLAS_NUM_THREADS", "1"},
        {"OMP_NUM_THREADS", "1"},
        {"MKL_NUM_THREADS", "1"},
        {"NUMEXPR_NUM_THREADS", "1"},
        {"GRPC_POLL_STRATEGY", "poll"}
      ],
      startup_batch_size: 8,
      startup_batch_delay_ms: 750,
      worker_ttl: :infinity,  # No automatic recycling
      worker_max_requests: :infinity
    },

    # CPU-intensive HPC pool (new pattern)
    %{
      name: :hpc_pool,
      worker_profile: :thread,
      pool_size: 4,  # 4 processes
      threads_per_worker: 16,  # 16 threads each = 64 total
      adapter_module: Snakepit.Adapters.GRPCPython,
      adapter_args: ["--mode", "threaded", "--max-workers", "16"],
      adapter_env: [
        # Allow multi-threading in libraries
        {"OPENBLAS_NUM_THREADS", "16"},
        {"OMP_NUM_THREADS", "16"}
      ],
      worker_ttl: {3600, :seconds},  # Recycle hourly
      worker_max_requests: 1000,     # Or after 1000 requests
      thread_safety_checks: true      # Enable runtime checks
    },

    # Dedicated pool for specific ML models
    %{
      name: :model_pool,
      worker_profile: :thread,
      pool_size: 2,
      threads_per_worker: 32,
      adapter_module: Snakepit.Adapters.GRPCPython,
      adapter_args: ["--adapter", "my_app.adapters.pytorch_adapter"],
      preload_resources: [
        {"/models/resnet50.pkl", :model},
        {"/models/bert-base.pkl", :model}
      ]
    }
  ]
```

### 3.2 Core Components

#### New Modules

```elixir
# 1. Worker profile abstraction
Snakepit.WorkerProfile
  - behaviour defining profile interface
  - ProcessProfile (existing multi-process)
  - ThreadProfile (new multi-threaded)

# 2. Pool registry enhancements
Snakepit.Pool.Registry
  - track multiple named pools
  - route requests to correct pool
  - support both profile types

# 3. Worker lifecycle management
Snakepit.Worker.LifecycleManager
  - TTL-based recycling
  - request count tracking
  - graceful shutdown coordination

# 4. Thread safety validation
Snakepit.ThreadSafety.Checker
  - runtime checks for thread-safe operations
  - warnings for non-thread-safe libraries
  - compatibility matrix

# 5. Diagnostic tooling
Snakepit.Diagnostics.ProfileInspector
  - per-profile metrics
  - thread vs process utilization
  - memory profiling
```

#### Modified Modules

```elixir
# Snakepit.Pool
  - support named pools
  - route to profile-specific implementation
  - load balancing across workers

# Snakepit.Adapters.GRPCPython
  - support both process and threaded modes
  - pass profile config to Python

# Snakepit.Pool.Worker.Starter
  - handle both profile types
  - coordinate lifecycle events
```

---

## 4. Implementation Roadmap

### Phase 1: Foundation (Weeks 1-2)

**Goal**: Core dual-mode infrastructure without breaking existing functionality

#### Tasks

1. **Worker Profile Behaviour** (`lib/snakepit/worker_profile.ex`)
   ```elixir
   defmodule Snakepit.WorkerProfile do
     @type profile_type :: :process | :thread

     @callback start_worker(config :: map()) :: {:ok, pid()} | {:error, term()}
     @callback stop_worker(worker :: pid()) :: :ok
     @callback execute_request(worker :: pid(), request :: map()) :: {:ok, term()} | {:error, term()}
     @callback get_capacity(worker :: pid()) :: non_neg_integer()
   end
   ```

2. **Process Profile Implementation** (`lib/snakepit/worker_profile/process.ex`)
   - Extract current multi-process logic
   - Implement WorkerProfile behaviour
   - Maintain 100% backward compatibility

3. **Thread Profile Stub** (`lib/snakepit/worker_profile/thread.ex`)
   - Basic implementation returning `:not_implemented`
   - Configuration parsing
   - Validation logic

4. **Pool Registry Refactor** (`lib/snakepit/pool/registry.ex`)
   ```elixir
   # Support named pools
   def via_tuple(pool_name, worker_id) when is_atom(pool_name)
   def get_worker(pool_name) when is_atom(pool_name)
   def list_pools() :: [atom()]
   ```

5. **Configuration Schema & Validation**
   - Add schema validation for pool configs
   - Validation errors for invalid profiles
   - Defaults for missing fields

6. **Testing**
   - All existing tests pass
   - Process profile tests
   - Configuration validation tests

### Phase 2: Multi-Threaded Python Worker (Weeks 3-4)

**Goal**: Implement the threaded Python gRPC server

#### Tasks

1. **Threaded gRPC Server** (`priv/python/grpc_server_threaded.py`)
   ```python
   import grpc
   from concurrent import futures
   import argparse

   class ThreadedSnakepitService(snakepit_pb2_grpc.SnakepitServicer):
       """Multi-threaded gRPC service with thread-safe adapter"""

       def __init__(self, adapter, thread_pool):
           self.adapter = adapter
           self.thread_pool = thread_pool
           self.request_count = 0
           self.lock = threading.Lock()

       def Execute(self, request, context):
           # Thread-safe request handling
           with self.lock:
               self.request_count += 1

           # Adapter methods MUST be thread-safe
           return self.adapter.handle_request(request)

   def serve(port: int, max_workers: int, adapter_spec: str):
       thread_pool = futures.ThreadPoolExecutor(max_workers=max_workers)
       server = grpc.server(
           thread_pool,
           options=[
               ('grpc.max_send_message_length', 100 * 1024 * 1024),
               ('grpc.max_receive_message_length', 100 * 1024 * 1024),
           ]
       )
       # ... rest of setup
   ```

2. **Thread-Safe Base Adapter** (`priv/python/snakepit_bridge/adapters/base_threaded.py`)
   ```python
   import threading
   from abc import ABC, abstractmethod

   class ThreadSafeAdapter(ABC):
       """Base adapter with thread safety guarantees"""

       def __init__(self):
           self._lock = threading.RLock()  # Reentrant lock
           self._request_count = 0

       def handle_request(self, request):
           """Thread-safe request dispatcher"""
           with self._lock:
               self._request_count += 1
               return self._dispatch(request)

       @abstractmethod
       def _dispatch(self, request):
           """Subclasses implement this - called within lock"""
           pass
   ```

3. **Example Threaded Adapter** (`priv/python/snakepit_bridge/adapters/threaded_showcase.py`)
   - Demonstrate thread-safe patterns
   - Use thread-local storage where appropriate
   - Show both locked and lockless patterns

4. **Adapter Detection CLI**
   ```bash
   # Enhance grpc_server.py CLI
   --mode {process|threaded}  # Required
   --max-workers N            # For threaded mode
   --thread-safety-check      # Enable runtime checks
   ```

5. **Testing**
   - Thread safety stress tests (concurrent requests)
   - Memory leak detection (long-running)
   - Race condition detection

### Phase 3: Elixir Thread Profile Implementation (Weeks 5-6)

**Goal**: Complete integration of thread profile in Elixir

#### Tasks

1. **Thread Profile Implementation** (`lib/snakepit/worker_profile/thread.ex`)
   ```elixir
   defmodule Snakepit.WorkerProfile.Thread do
     @behaviour Snakepit.WorkerProfile

     def start_worker(config) do
       # Start Python process with threaded mode
       # Track as multi-threaded worker
       # Return worker handle with capacity info
     end

     def get_capacity(worker) do
       # Return threads_per_worker from config
       # Allows pool to send N concurrent requests
     end
   end
   ```

2. **Pool Load Balancer Enhancement** (`lib/snakepit/pool/load_balancer.ex`)
   - Track in-flight requests per worker
   - Respect per-worker capacity limits
   - Route based on available capacity

3. **Request Multiplexing**
   - Allow N concurrent gRPC calls to one worker
   - Track concurrent request count
   - Queue when worker at capacity

4. **HTTP/2 Connection Pooling**
   - Reuse gRPC connections for threaded workers
   - Handle connection lifecycle
   - Reconnection logic

5. **Testing**
   - Concurrent request tests
   - Capacity limit tests
   - Connection pooling tests

### Phase 4: Worker Lifecycle Management (Week 7)

**Goal**: Automatic worker recycling and health management

#### Tasks

1. **Lifecycle Manager** (`lib/snakepit/worker/lifecycle_manager.ex`)
   ```elixir
   defmodule Snakepit.Worker.LifecycleManager do
     use GenServer

     def start_link(opts) do
       GenServer.start_link(__MODULE__, opts, name: __MODULE__)
     end

     def init(opts) do
       # Schedule periodic health checks
       schedule_health_check()
       {:ok, %{workers: %{}}}
     end

     def handle_info(:health_check, state) do
       # Check all workers
       state = check_worker_ttls(state)
       state = check_worker_request_counts(state)
       schedule_health_check()
       {:noreply, state}
     end

     defp recycle_worker(worker_id, reason) do
       # Gracefully shut down worker
       # Start replacement
       # Update registry
     end
   end
   ```

2. **TTL Tracking**
   - Track worker start time
   - Automatic recycling at TTL
   - Graceful handoff of in-flight requests

3. **Request Count Tracking**
   - Increment on each request
   - Recycle at max_requests
   - Per-worker counters

4. **Memory Monitoring** (Optional Enhancement)
   - Track Python process memory usage
   - Recycle on memory threshold
   - Telemetry events

5. **Testing**
   - TTL expiration tests
   - Request count recycling tests
   - Graceful shutdown tests

### Phase 5: Diagnostics & Monitoring (Week 8)

**Goal**: Enhanced observability for dual-mode operation

#### Tasks

1. **Profile Inspector** (`lib/snakepit/diagnostics/profile_inspector.ex`)
   ```elixir
   def get_pool_stats(pool_name) do
     %{
       profile: :thread,
       processes: 4,
       threads_per_process: 16,
       total_capacity: 64,
       active_requests: 23,
       available_capacity: 41,
       memory_per_process: "2.1 GB",
       total_memory: "8.4 GB"
     }
   end
   ```

2. **Enhanced `mix diagnose.scaling`**
   - Detect profile type
   - Report relevant metrics (processes vs threads)
   - Recommendations for tuning

3. **Telemetry Events**
   ```elixir
   [:snakepit, :worker, :started]
   [:snakepit, :worker, :recycled]
   [:snakepit, :worker, :capacity_saturated]
   [:snakepit, :pool, :profile_switch]  # For future dynamic switching
   ```

4. **LiveDashboard Integration** (Optional)
   - Real-time pool visualization
   - Per-profile metrics
   - Historical trends

5. **Testing**
   - Metrics accuracy tests
   - Telemetry event tests

### Phase 6: Documentation & Polish (Week 9-10)

**Goal**: Production-ready documentation and examples

#### Tasks

1. **Core Documentation**
   - Update README with dual-mode architecture
   - New guide: "Choosing Your Parallelism Model"
   - Configuration reference
   - Migration guide from v0.5.1

2. **Code Examples**
   - `examples/dual_mode_demo.exs` - Side-by-side comparison
   - `examples/hpc_pool_demo.exs` - CPU-intensive workload
   - `examples/hybrid_pools_demo.exs` - Multiple pool types

3. **Python Adapter Guide**
   - Writing thread-safe adapters
   - Common pitfalls
   - Testing for thread safety

4. **Performance Benchmarks**
   - Memory comparison (process vs thread)
   - Throughput comparison
   - Latency comparison
   - When to use which profile

5. **Compatibility Matrix**
   - Python versions (3.8-3.14+)
   - Library compatibility (NumPy, PyTorch, etc.)
   - Known thread-unsafe libraries

---

## 5. Configuration API

### High-Level Configuration

```elixir
# Simple single pool (backward compatible)
config :snakepit,
  pooling_enabled: true,
  adapter_module: Snakepit.Adapters.GRPCPython,
  pool_size: 8

# Multiple named pools (new)
config :snakepit,
  pools: [
    %{name: :default, worker_profile: :process, pool_size: 100},
    %{name: :hpc, worker_profile: :thread, pool_size: 4, threads_per_worker: 16}
  ]
```

### Per-Pool Configuration Options

```elixir
%{
  # Required
  name: atom(),                           # Pool identifier
  worker_profile: :process | :thread,     # Profile type

  # Common to both profiles
  adapter_module: module(),               # Adapter to use
  adapter_args: [String.t()],             # CLI args for Python
  adapter_env: [{String.t(), String.t()}], # Environment variables

  # Process profile specific
  pool_size: pos_integer(),               # Number of workers
  startup_batch_size: pos_integer(),      # Workers per batch
  startup_batch_delay_ms: pos_integer(),  # Delay between batches

  # Thread profile specific
  pool_size: pos_integer(),               # Number of processes
  threads_per_worker: pos_integer(),      # Threads per process
  thread_safety_checks: boolean(),        # Enable safety checks

  # Lifecycle management
  worker_ttl: :infinity | {integer(), :seconds | :minutes | :hours},
  worker_max_requests: :infinity | pos_integer(),

  # Advanced
  preload_resources: [{path(), type()}],  # Preload models/data
  shared_memory_size: nil | integer(),    # For future zero-copy
}
```

### Runtime API

```elixir
# Execute on default pool (backward compatible)
Snakepit.execute("command", %{args: "here"})

# Execute on named pool (new)
Snakepit.execute(:hpc_pool, "compute", %{matrix: data})

# Get pool statistics
Snakepit.Pool.get_stats(:hpc_pool)
# => %{profile: :thread, processes: 4, threads_per_process: 16, ...}

# List available pools
Snakepit.Pool.list_pools()
# => [:default, :hpc_pool, :model_pool]

# Recycle a worker manually
Snakepit.Pool.recycle_worker(:hpc_pool, worker_id)
```

---

## 6. Python Worker Implementations

### 6.1 Process-Based Worker (Existing)

```python
# priv/python/grpc_server.py (enhanced)

def serve_process_mode(port: int, adapter_spec: str):
    """Single-threaded gRPC server (existing pattern)"""

    # Single-threaded environment
    os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
    os.environ.setdefault("OMP_NUM_THREADS", "1")

    # Single-threaded gRPC
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=1))
    adapter = load_adapter(adapter_spec)

    snakepit_pb2_grpc.add_SnakepitServicer_to_server(
        SnakepitService(adapter), server
    )

    server.add_insecure_port(f'[::]:{port}')
    server.start()
    server.wait_for_termination()
```

### 6.2 Thread-Based Worker (New)

```python
# priv/python/grpc_server_threaded.py

import grpc
from concurrent import futures
import threading
import logging
from typing import Optional

class ThreadedSnakepitService(snakepit_pb2_grpc.SnakepitServicer):
    """Multi-threaded gRPC service with comprehensive safety"""

    def __init__(self, adapter, max_workers: int):
        self.adapter = adapter
        self.max_workers = max_workers

        # Request tracking
        self.request_count = 0
        self.active_requests = 0
        self.lock = threading.RLock()

        # Performance tracking
        self.start_time = time.time()

    def Execute(self, request, context):
        """Thread-safe request execution"""
        request_id = None

        try:
            with self.lock:
                self.request_count += 1
                self.active_requests += 1
                request_id = self.request_count

            logging.debug(f"[Thread {threading.current_thread().name}] "
                         f"Request {request_id} starting")

            # Adapter handles actual work (must be thread-safe!)
            result = self.adapter.handle_request(request)

            logging.debug(f"[Thread {threading.current_thread().name}] "
                         f"Request {request_id} completed")

            return result

        except Exception as e:
            logging.error(f"Request {request_id} failed: {e}", exc_info=True)
            raise

        finally:
            with self.lock:
                self.active_requests -= 1

    def get_stats(self):
        """Thread-safe statistics"""
        with self.lock:
            uptime = time.time() - self.start_time
            return {
                'total_requests': self.request_count,
                'active_requests': self.active_requests,
                'max_workers': self.max_workers,
                'uptime_seconds': uptime
            }

def serve_threaded_mode(port: int, max_workers: int, adapter_spec: str):
    """Multi-threaded gRPC server"""

    logging.info(f"Starting threaded gRPC server: port={port}, "
                 f"max_workers={max_workers}")

    # Allow multi-threading in libraries (user-controlled via config)
    # DO NOT set OPENBLAS_NUM_THREADS here - let Elixir config control it

    # Create thread pool
    thread_pool = futures.ThreadPoolExecutor(max_workers=max_workers)

    # Create server with large message sizes
    server = grpc.server(
        thread_pool,
        options=[
            ('grpc.max_send_message_length', 100 * 1024 * 1024),
            ('grpc.max_receive_message_length', 100 * 1024 * 1024),
            ('grpc.max_concurrent_streams', max_workers),
        ]
    )

    # Load adapter (must be thread-safe!)
    adapter = load_adapter(adapter_spec)
    if not hasattr(adapter, '__thread_safe__'):
        logging.warning(f"Adapter {adapter_spec} does not declare thread safety. "
                       f"This may cause race conditions!")

    # Add service
    service = ThreadedSnakepitService(adapter, max_workers)
    snakepit_pb2_grpc.add_SnakepitServicer_to_server(service, server)

    # Start server
    server.add_insecure_port(f'[::]:{port}')
    server.start()

    logging.info(f"Threaded gRPC server listening on port {port}")

    # Graceful shutdown on signals
    def shutdown_handler(signum, frame):
        logging.info(f"Received signal {signum}, shutting down...")
        stats = service.get_stats()
        logging.info(f"Final stats: {stats}")
        server.stop(grace=5.0)

    signal.signal(signal.SIGTERM, shutdown_handler)
    signal.signal(signal.SIGINT, shutdown_handler)

    server.wait_for_termination()

# Enhanced CLI
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", choices=["process", "threaded"],
                       default="process")
    parser.add_argument("--port", type=int, default=0)
    parser.add_argument("--max-workers", type=int, default=10)
    parser.add_argument("--adapter", default="snakepit_bridge.adapters.showcase")

    args = parser.parse_args()

    if args.mode == "threaded":
        serve_threaded_mode(args.port, args.max_workers, args.adapter)
    else:
        serve_process_mode(args.port, args.adapter)
```

### 6.3 Thread-Safe Adapter Base Class

```python
# priv/python/snakepit_bridge/adapters/base_threaded.py

import threading
import logging
from abc import ABC, abstractmethod
from typing import Any, Dict
from contextlib import contextmanager

class ThreadSafeAdapter(ABC):
    """
    Base class for thread-safe adapters.

    Provides infrastructure for:
    - Thread-local storage
    - Request-scoped locking
    - Safe resource management
    """

    __thread_safe__ = True  # Marker for safety validation

    def __init__(self):
        self._lock = threading.RLock()  # Reentrant lock
        self._thread_local = threading.local()
        self._request_count = 0
        self._logger = logging.getLogger(self.__class__.__name__)

    def handle_request(self, request) -> Any:
        """Thread-safe request handler - DO NOT override"""
        with self._track_request():
            return self._handle_request_impl(request)

    @abstractmethod
    def _handle_request_impl(self, request) -> Any:
        """Subclasses implement this - called within tracking context"""
        pass

    @contextmanager
    def _track_request(self):
        """Context manager for request tracking"""
        with self._lock:
            self._request_count += 1
            request_id = self._request_count

        self._logger.debug(f"Request {request_id} starting")
        try:
            yield
        finally:
            self._logger.debug(f"Request {request_id} completed")

    @contextmanager
    def acquire_lock(self):
        """Explicitly acquire the adapter lock for critical sections"""
        with self._lock:
            yield

    def get_thread_local(self, key: str, default=None):
        """Get thread-local value"""
        return getattr(self._thread_local, key, default)

    def set_thread_local(self, key: str, value: Any):
        """Set thread-local value"""
        setattr(self._thread_local, key, value)


# Example: Thread-safe ML adapter
class ThreadSafeMLAdapter(ThreadSafeAdapter):
    """
    Example adapter showing thread-safe patterns.

    Pattern 1: Shared read-only resources (models)
    Pattern 2: Thread-local caches
    Pattern 3: Locked write operations
    """

    def __init__(self):
        super().__init__()

        # Shared read-only: Safe to access from any thread
        self.model = self._load_model()  # Load once

        # Shared writable: Protected by lock
        self.request_log = []

    def _load_model(self):
        """Load model once - shared across threads"""
        # This is safe because the model is read-only after loading
        import torch
        return torch.load("/models/resnet50.pkl")

    def _handle_request_impl(self, request):
        """Handle request using thread-safe patterns"""

        # Pattern 1: Use shared read-only resource (no lock needed)
        prediction = self.model.forward(request.input)

        # Pattern 2: Use thread-local cache
        cache = self.get_thread_local('cache')
        if cache is None:
            cache = {}
            self.set_thread_local('cache', cache)

        # Pattern 3: Write to shared state (MUST lock)
        with self.acquire_lock():
            self.request_log.append({
                'timestamp': time.time(),
                'result': prediction
            })

        return prediction
```

---

## 7. Elixir Pool Enhancements

### 7.1 Worker Profile Behaviour

```elixir
# lib/snakepit/worker_profile.ex

defmodule Snakepit.WorkerProfile do
  @moduledoc """
  Behaviour for worker profiles (process vs thread).

  A worker profile defines how workers are created, managed, and utilized.
  """

  @type worker_handle :: pid() | reference()
  @type config :: map()
  @type capacity :: pos_integer()

  @doc "Start a worker with the given configuration"
  @callback start_worker(config) :: {:ok, worker_handle} | {:error, term()}

  @doc "Stop a worker gracefully"
  @callback stop_worker(worker_handle) :: :ok

  @doc "Execute a request on a worker"
  @callback execute_request(worker_handle, request :: map(), timeout :: timeout()) ::
    {:ok, term()} | {:error, term()}

  @doc "Get the current capacity of a worker (how many concurrent requests it can handle)"
  @callback get_capacity(worker_handle) :: capacity()

  @doc "Get the current load of a worker (how many requests are in-flight)"
  @callback get_load(worker_handle) :: non_neg_integer()

  @doc "Check if a worker is healthy"
  @callback health_check(worker_handle) :: :ok | {:error, term()}
end
```

### 7.2 Thread Profile Implementation

```elixir
# lib/snakepit/worker_profile/thread.ex

defmodule Snakepit.WorkerProfile.Thread do
  @moduledoc """
  Multi-threaded worker profile.

  Each worker is a Python process with a thread pool, capable of handling
  multiple concurrent requests via HTTP/2 multiplexing.
  """

  @behaviour Snakepit.WorkerProfile

  alias Snakepit.Pool.GRPCWorker

  @impl true
  def start_worker(config) do
    # Enhance config for threaded mode
    adapter_args = build_adapter_args(config)
    adapter_env = build_adapter_env(config)

    worker_config = config
    |> Map.put(:adapter_args, adapter_args)
    |> Map.put(:adapter_env, adapter_env)
    |> Map.put(:profile, :thread)

    # Start the worker (enhanced GRPCWorker handles threaded mode)
    case GRPCWorker.start_link(worker_config) do
      {:ok, pid} ->
        # Track capacity
        capacity = Map.get(config, :threads_per_worker, 10)
        :ets.insert(:worker_capacity, {pid, capacity, 0})
        {:ok, pid}

      error -> error
    end
  end

  @impl true
  def stop_worker(worker_pid) do
    GRPCWorker.stop(worker_pid)
  end

  @impl true
  def execute_request(worker_pid, request, timeout) do
    # Check capacity before sending
    case check_capacity(worker_pid) do
      :ok ->
        # Track in-flight request
        :ets.update_counter(:worker_capacity, worker_pid, {3, 1})

        try do
          GRPCWorker.execute(worker_pid, request, timeout)
        after
          # Decrement in-flight count
          :ets.update_counter(:worker_capacity, worker_pid, {3, -1})
        end

      {:error, :at_capacity} ->
        {:error, :worker_saturated}
    end
  end

  @impl true
  def get_capacity(worker_pid) do
    case :ets.lookup(:worker_capacity, worker_pid) do
      [{^worker_pid, capacity, _load}] -> capacity
      [] -> 1  # Default if not found
    end
  end

  @impl true
  def get_load(worker_pid) do
    case :ets.lookup(:worker_capacity, worker_pid) do
      [{^worker_pid, _capacity, load}] -> load
      [] -> 0
    end
  end

  @impl true
  def health_check(worker_pid) do
    GRPCWorker.ping(worker_pid)
  end

  # Private helpers

  defp build_adapter_args(config) do
    threads = Map.get(config, :threads_per_worker, 10)
    adapter = Map.get(config, :adapter_spec, "snakepit_bridge.adapters.showcase")

    base_args = [
      "--mode", "threaded",
      "--max-workers", "#{threads}",
      "--adapter", adapter
    ]

    # Merge with user-provided args
    user_args = Map.get(config, :adapter_args, [])
    base_args ++ user_args
  end

  defp build_adapter_env(config) do
    threads = Map.get(config, :threads_per_worker, 10)

    # Default env for threaded mode (user can override)
    default_env = [
      {"OPENBLAS_NUM_THREADS", "#{threads}"},
      {"OMP_NUM_THREADS", "#{threads}"},
      {"MKL_NUM_THREADS", "#{threads}"}
    ]

    user_env = Map.get(config, :adapter_env, [])

    # User env overrides defaults
    Enum.uniq_by(user_env ++ default_env, fn {key, _} -> key end)
  end

  defp check_capacity(worker_pid) do
    case :ets.lookup(:worker_capacity, worker_pid) do
      [{^worker_pid, capacity, load}] when load < capacity -> :ok
      [{^worker_pid, _capacity, _load}] -> {:error, :at_capacity}
      [] -> :ok  # Unknown worker, allow
    end
  end
end
```

### 7.3 Enhanced Pool Manager

```elixir
# lib/snakepit/pool.ex (enhancements)

defmodule Snakepit.Pool do
  use GenServer

  @moduledoc """
  Enhanced pool manager with multi-profile support.
  """

  # ... existing code ...

  def init(opts) do
    pools_config = Keyword.get(opts, :pools, [default_pool_config()])

    # Start each named pool
    state = Enum.reduce(pools_config, %{pools: %{}}, fn pool_config, state ->
      pool_name = Map.fetch!(pool_config, :name)

      # Determine profile implementation
      profile_module = case Map.get(pool_config, :worker_profile, :process) do
        :process -> Snakepit.WorkerProfile.Process
        :thread -> Snakepit.WorkerProfile.Thread
      end

      # Start workers for this pool
      workers = start_pool_workers(pool_name, profile_module, pool_config)

      pool_state = %{
        name: pool_name,
        profile: profile_module,
        config: pool_config,
        workers: workers
      }

      put_in(state, [:pools, pool_name], pool_state)
    end)

    {:ok, state}
  end

  # Enhanced execute with pool routing
  def execute(pool_name \\ :default, command, args, opts \\ []) do
    GenServer.call(__MODULE__, {:execute, pool_name, command, args, opts})
  end

  def handle_call({:execute, pool_name, command, args, opts}, from, state) do
    case get_in(state, [:pools, pool_name]) do
      nil ->
        {:reply, {:error, {:unknown_pool, pool_name}}, state}

      pool_state ->
        # Get worker using profile-aware load balancing
        case get_available_worker(pool_state) do
          {:ok, worker} ->
            # Execute using profile's execute_request
            profile = pool_state.profile
            timeout = Keyword.get(opts, :timeout, 30_000)

            request = %{command: command, args: args}

            # Async execution to avoid blocking pool
            Task.start(fn ->
              result = profile.execute_request(worker, request, timeout)
              GenServer.reply(from, result)
            end)

            {:noreply, state}

          {:error, :no_workers_available} ->
            {:reply, {:error, :pool_saturated}, state}
        end
    end
  end

  defp get_available_worker(pool_state) do
    profile = pool_state.profile

    # Find worker with available capacity
    available = Enum.find(pool_state.workers, fn worker ->
      load = profile.get_load(worker)
      capacity = profile.get_capacity(worker)
      load < capacity
    end)

    case available do
      nil -> {:error, :no_workers_available}
      worker -> {:ok, worker}
    end
  end
end
```

---

## 8. Advanced Features

### 8.1 Worker Recycling

```elixir
# lib/snakepit/worker/lifecycle_manager.ex

defmodule Snakepit.Worker.LifecycleManager do
  use GenServer
  require Logger

  @check_interval 60_000  # Check every minute

  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end

  def init(_opts) do
    schedule_check()
    {:ok, %{workers: %{}}}
  end

  def track_worker(pool_name, worker_pid, config) do
    GenServer.cast(__MODULE__, {:track, pool_name, worker_pid, config})
  end

  def handle_cast({:track, pool_name, worker_pid, config}, state) do
    worker_state = %{
      pool: pool_name,
      pid: worker_pid,
      started_at: System.monotonic_time(:second),
      request_count: 0,
      ttl: parse_ttl(config[:worker_ttl]),
      max_requests: config[:worker_max_requests] || :infinity
    }

    state = put_in(state, [:workers, worker_pid], worker_state)
    Process.monitor(worker_pid)

    {:noreply, state}
  end

  def handle_info(:check, state) do
    now = System.monotonic_time(:second)

    # Check each worker
    state = Enum.reduce(state.workers, state, fn {pid, worker_state}, acc_state ->
      cond do
        should_recycle_ttl?(worker_state, now) ->
          Logger.info("Recycling worker #{inspect(pid)} - TTL exceeded")
          recycle_worker(worker_state)
          Map.delete(acc_state.workers, pid)

        should_recycle_requests?(worker_state) ->
          Logger.info("Recycling worker #{inspect(pid)} - max requests reached")
          recycle_worker(worker_state)
          Map.delete(acc_state.workers, pid)

        true ->
          acc_state
      end
    end)

    schedule_check()
    {:noreply, %{state | workers: state.workers}}
  end

  def handle_info({:DOWN, _ref, :process, pid, reason}, state) do
    Logger.debug("Worker #{inspect(pid)} exited: #{inspect(reason)}")
    {:noreply, %{state | workers: Map.delete(state.workers, pid)}}
  end

  defp should_recycle_ttl?(worker_state, now) do
    case worker_state.ttl do
      :infinity -> false
      ttl_seconds -> (now - worker_state.started_at) >= ttl_seconds
    end
  end

  defp should_recycle_requests?(worker_state) do
    case worker_state.max_requests do
      :infinity -> false
      max -> worker_state.request_count >= max
    end
  end

  defp recycle_worker(worker_state) do
    # Stop the old worker
    Snakepit.Pool.stop_worker(worker_state.pool, worker_state.pid)

    # Start a replacement
    Snakepit.Pool.add_worker(worker_state.pool)
  end

  defp parse_ttl(:infinity), do: :infinity
  defp parse_ttl({value, :seconds}), do: value
  defp parse_ttl({value, :minutes}), do: value * 60
  defp parse_ttl({value, :hours}), do: value * 3600

  defp schedule_check do
    Process.send_after(self(), :check, @check_interval)
  end
end
```

### 8.2 Thread Safety Validation

```python
# priv/python/snakepit_bridge/thread_safety/checker.py

import threading
import warnings
from typing import Set
import inspect

class ThreadSafetyChecker:
    """
    Runtime thread safety validation.

    Detects:
    - Concurrent access without locking
    - Known thread-unsafe libraries
    - Race conditions
    """

    # Known thread-unsafe libraries
    UNSAFE_LIBRARIES = {
        'matplotlib': 'Use thread-local figures or lock access',
        'sqlite3': 'Use check_same_thread=False with caution',
    }

    def __init__(self, adapter):
        self.adapter = adapter
        self.access_map: dict[str, Set[int]] = {}
        self.lock = threading.Lock()
        self.warnings_issued: Set[str] = set()

    def check_method_access(self, method_name: str):
        """Check if a method is being accessed concurrently"""
        thread_id = threading.get_ident()

        with self.lock:
            if method_name not in self.access_map:
                self.access_map[method_name] = set()

            self.access_map[method_name].add(thread_id)

            # Warn if multiple threads accessing without locking
            if len(self.access_map[method_name]) > 1:
                if method_name not in self.warnings_issued:
                    warnings.warn(
                        f"Method {method_name} accessed from multiple threads "
                        f"without explicit locking. This may cause race conditions.",
                        RuntimeWarning
                    )
                    self.warnings_issued.add(method_name)

    def check_library_safety(self):
        """Check if any imported libraries are known to be thread-unsafe"""
        import sys

        for lib_name, warning_msg in self.UNSAFE_LIBRARIES.items():
            if lib_name in sys.modules:
                warnings.warn(
                    f"Thread-unsafe library detected: {lib_name}. {warning_msg}",
                    RuntimeWarning
                )

# Decorator for methods
def thread_safe_method(func):
    """Decorator to mark methods as explicitly thread-safe"""
    func.__thread_safe__ = True
    return func

# Usage in adapter
class MyAdapter(ThreadSafeAdapter):

    def __init__(self):
        super().__init__()
        if self.config.get('thread_safety_checks'):
            self.checker = ThreadSafetyChecker(self)

    @thread_safe_method
    def handle_request(self, request):
        if hasattr(self, 'checker'):
            self.checker.check_method_access('handle_request')

        # ... implementation
```

---

## 9. Testing Strategy

### 9.1 Test Matrix

```
Profile   | Python | Concurrency | Load    | Focus
----------|--------|-------------|---------|---------------------------
:process  | 3.8    | 100 req/s   | Light   | Backward compatibility
:process  | 3.12   | 1000 req/s  | Heavy   | Stability under load
:thread   | 3.13   | 100 req/s   | Light   | Basic thread safety
:thread   | 3.13   | 1000 req/s  | Heavy   | Race condition detection
:thread   | 3.14   | 1000 req/s  | Heavy   | Free-threading mode
```

### 9.2 Test Cases

```elixir
# test/snakepit/worker_profile_test.exs

defmodule Snakepit.WorkerProfileTest do
  use ExUnit.Case, async: false

  describe "Process profile" do
    test "maintains backward compatibility" do
      config = %{
        name: :test,
        worker_profile: :process,
        pool_size: 4
      }

      {:ok, _pid} = start_supervised({Snakepit.Pool, pools: [config]})

      # All existing API calls work
      assert {:ok, _} = Snakepit.execute("ping", %{})
      assert {:ok, _} = Snakepit.execute("compute", %{a: 5, b: 3})
    end

    test "enforces single-threading" do
      # Verify OPENBLAS_NUM_THREADS=1 is set
      # Check that only 1 thread is created per process
    end
  end

  describe "Thread profile" do
    test "handles concurrent requests" do
      config = %{
        name: :threaded_test,
        worker_profile: :thread,
        pool_size: 2,
        threads_per_worker: 4
      }

      {:ok, _pid} = start_supervised({Snakepit.Pool, pools: [config]})

      # Send 8 concurrent requests (should use both workers)
      tasks = for i <- 1..8 do
        Task.async(fn ->
          Snakepit.execute(:threaded_test, "compute", %{value: i})
        end)
      end

      results = Task.await_many(tasks, 10_000)
      assert length(results) == 8
      assert Enum.all?(results, fn {:ok, _} -> true; _ -> false end)
    end

    test "respects worker capacity limits" do
      # Fill up worker capacity, verify queue behavior
    end

    test "detects thread safety violations" do
      # Use adapter with race condition, verify warning
    end
  end

  describe "Worker recycling" do
    test "recycles worker after TTL" do
      config = %{
        name: :ttl_test,
        worker_profile: :process,
        pool_size: 1,
        worker_ttl: {5, :seconds}
      }

      {:ok, _pid} = start_supervised({Snakepit.Pool, pools: [config]})

      # Get initial worker
      {:ok, worker1} = get_worker(:ttl_test)

      # Wait for TTL + check interval
      Process.sleep(6_500)

      # Worker should be different
      {:ok, worker2} = get_worker(:ttl_test)
      assert worker1 != worker2
    end

    test "recycles worker after max requests" do
      # Similar test for request count recycling
    end
  end
end
```

### 9.3 Thread Safety Tests

```python
# tests/test_thread_safety.py

import pytest
import threading
import time
from concurrent.futures import ThreadPoolExecutor

def test_concurrent_requests(threaded_adapter):
    """Test adapter under concurrent load"""
    results = []
    errors = []

    def make_request(i):
        try:
            result = threaded_adapter.handle_request({
                'command': 'compute',
                'args': {'value': i}
            })
            results.append(result)
        except Exception as e:
            errors.append(e)

    # Hammer it with 100 concurrent requests
    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = [executor.submit(make_request, i) for i in range(100)]
        for future in futures:
            future.result(timeout=10)

    # All should succeed
    assert len(results) == 100
    assert len(errors) == 0

def test_race_condition_detection(unsafe_adapter):
    """Test that checker detects race conditions"""

    with pytest.warns(RuntimeWarning, match="multiple threads"):
        # Create race condition
        def access_shared_state():
            unsafe_adapter.shared_counter += 1

        threads = [threading.Thread(target=access_shared_state) for _ in range(10)]
        for t in threads:
            t.start()
        for t in threads:
            t.join()
```

---

## 10. Documentation Plan

### 10.1 New Documentation Files

1. **`docs/parallelism_guide.md`** - Choosing Your Parallelism Model (2000 words)
2. **`docs/thread_safety_guide.md`** - Writing Thread-Safe Adapters (1500 words)
3. **`docs/configuration_reference.md`** - Complete config reference (3000 words)
4. **`docs/migration_v0.6.md`** - Migration guide from v0.5.1 (1000 words)
5. **`docs/performance_benchmarks.md`** - Detailed benchmarks (1500 words)

### 10.2 README Updates

Add sections:
- **Dual-Mode Architecture** (overview)
- **Quick Start for Both Modes**
- **When to Use Which Profile**
- **Performance Comparison**

### 10.3 Example Scripts

```elixir
# examples/profile_comparison_demo.exs
# Side-by-side comparison of both profiles

# examples/hpc_workload_demo.exs
# CPU-intensive workload on thread profile

# examples/hybrid_pools_demo.exs
# Multiple pools with different profiles

# examples/worker_recycling_demo.exs
# Demonstrate TTL and request-count recycling
```

---

## 11. Migration Guide

### For Existing Users (v0.5.1 → v0.6.0)

**No changes required!** Your existing configuration continues to work:

```elixir
# This still works exactly as before
config :snakepit,
  pooling_enabled: true,
  adapter_module: Snakepit.Adapters.GRPCPython,
  pool_size: 100
```

### To Adopt Thread Profile

```elixir
# Add a new pool with thread profile
config :snakepit,
  pools: [
    # Keep existing behavior for API workload
    %{
      name: :default,
      worker_profile: :process,
      pool_size: 100
    },

    # Add thread profile for CPU-intensive work
    %{
      name: :compute,
      worker_profile: :thread,
      pool_size: 4,
      threads_per_worker: 16
    }
  ]

# Use it
Snakepit.execute(:compute, "train_model", %{...})
```

---

## 12. Performance Benchmarks

### Expected Results

**Memory Consumption**

```
Profile      | Workers | Memory per Worker | Total Memory
-------------|---------|-------------------|-------------
:process     | 100     | 150 MB            | 15 GB
:thread      | 4       | 400 MB            | 1.6 GB
Savings      |         |                   | 9.4× reduction
```

**Throughput (Small Tasks)**

```
Profile      | Workers | Requests/sec | Notes
-------------|---------|--------------|---------------------------
:process     | 100     | 1500         | Current v0.5.1 performance
:thread      | 4 × 16  | 1200         | Slightly lower (overhead)
```

**Throughput (CPU-Intensive)**

```
Profile      | Workers | Jobs/hour | Notes
-------------|---------|-----------|---------------------------
:process     | 100     | 600       | Single-threaded per worker
:thread      | 4 × 16  | 2400      | 4× improvement via multi-threading
```

### Benchmark Scenarios

1. **Small ML Inference** (API use case)
2. **Large Data Processing** (CPU use case)
3. **Mixed Workload** (Both profiles)
4. **Memory Pressure** (Long-running)
5. **Startup Time** (Cold start)

---

## 13. Risk Mitigation

### Risk Matrix

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Thread safety bugs | Medium | High | Extensive testing, runtime checks, clear docs |
| Backward compatibility break | Low | Critical | Strict testing, default to :process |
| Python 3.13+ stability | Medium | Medium | Support both modes, conservative defaults |
| Performance regression | Low | High | Comprehensive benchmarks, A/B testing |
| Configuration complexity | Medium | Low | Good defaults, clear documentation |
| Library incompatibility | High | Medium | Compatibility matrix, automatic detection |

### Mitigation Strategies

1. **Extensive Testing** - 9 weeks dedicated to testing
2. **Feature Flags** - Allow disabling thread profile if issues arise
3. **Rollback Plan** - Can revert to :process-only if critical bug found
4. **Conservative Defaults** - :process remains default
5. **Clear Documentation** - Explicit guidance on thread safety
6. **Runtime Validation** - Thread safety checker in dev mode

---

## 14. Future Directions (Post-v0.6.0)

### Phase 7: Zero-Copy Data Transfer (v0.7.0)

Use shared memory for large tensors:

```elixir
# Elixir side: Write to shared memory
{:ok, shm} = SharedMemory.create("tensor_#{id}", size_bytes)
SharedMemory.write(shm, tensor_data)

# Pass handle to Python
Snakepit.execute(:hpc_pool, "process_tensor", %{shm_handle: shm.handle})

# Python side: Read from shared memory without copying
import mmap
tensor = np.frombuffer(mmap.mmap(shm_fd, size), dtype=np.float32)
```

### Phase 8: Dynamic Profile Switching (v0.8.0)

Automatically switch profiles based on workload:

```elixir
config :snakepit,
  adaptive_pooling: true,
  switch_threshold: %{
    cpu_usage: 0.8,
    memory_usage: 0.9
  }
```

### Phase 9: Distributed Worker Pools (v0.9.0)

Workers across multiple machines:

```elixir
config :snakepit,
  distributed: true,
  nodes: ["node1@host", "node2@host"]
```

---

## 15. Success Criteria

### v0.6.0 is successful if:

1. ✅ **Zero breaking changes** - All v0.5.1 code works without modification
2. ✅ **Thread profile works** - Multi-threaded workers handle concurrent requests
3. ✅ **Performance gains** - Thread profile shows ≥3× improvement on CPU workloads
4. ✅ **Memory savings** - Thread profile uses ≤20% memory of process profile
5. ✅ **Stable** - No thread safety issues in 1 week of production testing
6. ✅ **Well-documented** - Users can choose profile confidently
7. ✅ **Test coverage** - ≥90% coverage on new code
8. ✅ **Example quality** - All examples work and demonstrate value

---

## 16. Timeline Summary

| Phase | Duration | Deliverable |
|-------|----------|-------------|
| 1. Foundation | 2 weeks | Worker profile behaviour, config schema |
| 2. Python Worker | 2 weeks | Threaded gRPC server, thread-safe adapters |
| 3. Elixir Integration | 2 weeks | Thread profile implementation, load balancing |
| 4. Lifecycle Mgmt | 1 week | Worker recycling, TTL, health checks |
| 5. Diagnostics | 1 week | Enhanced monitoring, telemetry |
| 6. Documentation | 2 weeks | Guides, examples, benchmarks |
| **Total** | **10 weeks** | **Production-ready v0.6.0** |

---

## 17. Design Additions & Changes

Based on the analysis, I recommend these additional enhancements:

### 17.1 Environment Variable Expansion

Add comprehensive thread control for all libraries:

```elixir
# In config validation
@all_thread_vars [
  "OPENBLAS_NUM_THREADS",
  "MKL_NUM_THREADS",
  "OMP_NUM_THREADS",
  "NUMEXPR_NUM_THREADS",
  "VECLIB_MAXIMUM_THREADS",  # macOS
  "GRPC_POLL_STRATEGY"
]

def default_process_env do
  Enum.map(@all_thread_vars, fn var -> {var, "1"} end)
end
```

### 17.2 Python Version Detection

```elixir
# lib/snakepit/python_version.ex

defmodule Snakepit.PythonVersion do
  def detect do
    case System.cmd("python3", ["--version"]) do
      {output, 0} ->
        parse_version(output)
      _ ->
        {:error, :python_not_found}
    end
  end

  def supports_free_threading?({major, minor, _patch}) do
    major == 3 and minor >= 13
  end

  def recommend_profile do
    case detect() do
      {:ok, version} ->
        if supports_free_threading?(version) do
          :thread
        else
          :process
        end
      _ ->
        :process  # Safe default
    end
  end
end
```

### 17.3 Compatibility Matrix as Data

```elixir
# lib/snakepit/compatibility.ex

@library_compatibility %{
  "numpy" => %{
    thread_safe: true,
    notes: "Releases GIL during computation",
    min_version: "1.20.0"
  },
  "pandas" => %{
    thread_safe: false,
    notes: "Not thread-safe as of v2.0",
    recommendation: "Use process profile"
  },
  "torch" => %{
    thread_safe: true,
    notes: "Thread-safe with proper initialization",
    min_version: "2.0.0"
  },
  "matplotlib" => %{
    thread_safe: false,
    notes: "Use thread-local figures",
    workaround: "See docs/thread_safety_guide.md"
  }
}

def check_compatibility(library_name, worker_profile) do
  case Map.get(@library_compatibility, library_name) do
    nil -> {:unknown, "Compatibility unknown"}
    %{thread_safe: true} -> :ok
    %{thread_safe: false} when worker_profile == :thread ->
      {:warning, Map.get(info, :notes)}
    _ -> :ok
  end
end
```

### 17.4 Graceful Degradation

```elixir
# If thread profile fails to start, fall back to process
def start_worker(config) do
  case do_start_worker(config) do
    {:ok, worker} ->
      {:ok, worker}

    {:error, reason} when config.worker_profile == :thread ->
      Logger.warning("Thread profile failed (#{inspect(reason)}), " <>
                    "falling back to process profile")

      process_config = %{config | worker_profile: :process}
      Snakepit.WorkerProfile.Process.start_worker(process_config)

    error ->
      error
  end
end
```

---

## Conclusion

This technical plan provides a **comprehensive, cohesive roadmap** for implementing dual-mode parallelism in Snakepit v0.6.0. The architecture:

1. ✅ **Preserves backward compatibility** - Existing users unaffected
2. ✅ **Enables future workloads** - Python 3.13+ free-threading support
3. ✅ **Provides explicit control** - Users choose the right profile
4. ✅ **Delivers robust defaults** - Safe, production-ready out of the box
5. ✅ **Scales appropriately** - Both high-concurrency and high-CPU workloads
6. ✅ **Maintains quality** - Comprehensive testing and documentation
7. ✅ **Positions Snakepit as world-class** - Industry-leading Elixir/Python bridge

The 10-week timeline is aggressive but achievable. The phased approach allows for course correction and ensures each component is solid before building the next layer.

**Recommendation: Proceed with implementation starting with Phase 1.**
