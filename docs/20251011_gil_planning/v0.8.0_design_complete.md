# v0.8.0: Zero-Copy Data Transfer - Complete Design & Architecture

**Status:** Design Phase
**Version:** 0.8.0
**Date:** 2025-10-11
**Prerequisites:** v0.7.0 (Python Environment Management)
**Estimated Effort:** 5-6 weeks

---

## Executive Summary

v0.8.0 introduces **zero-copy data transfer** between BEAM and Python using shared memory, eliminating serialization overhead for large data workloads.

**Key Benefits:**
- **50x faster** for 100MB+ data transfers
- **70% less memory** (single copy vs multiple)
- **Transparent** automatic threshold switching
- **Safe** concurrent access with v0.6.0 thread profiles

---

## Problem & Solution

### Current Architecture (v0.7.0)

```
Elixir: 100MB tensor
  ↓ Serialize to Protobuf (copy #1)
100MB on wire
  ↓ Send via gRPC
Python process
  ↓ Deserialize (copy #2)
100MB NumPy array

Total: 300MB memory, ~50ms latency
```

### v0.8.0 Zero-Copy Architecture

```
Elixir: 100MB tensor
  ↓ Write to shared memory (copy #1)
Shared Memory Region
  ↓ Memory-map (zero-copy)
Python process reads directly
  ↓ View as NumPy array (zero-copy)
100MB NumPy array

Total: 100MB memory, ~2ms latency
```

---

## Design Philosophy

### 1. Transparent API

Users shouldn't know about shared memory:

```elixir
# Same API for small and large data
Snakepit.execute("process", %{
  image: large_tensor  # Auto-uses shared memory if >10MB
})
```

### 2. Automatic Threshold Switching

```
Data Size < 10MB  → gRPC Protobuf (fast enough)
Data Size ≥ 10MB  → Shared memory (worth the overhead)
```

### 3. Safety First

- Memory fencing for concurrent access
- Automatic cleanup on worker crash
- Process isolation maintained

### 4. Incremental Adoption

```elixir
# Opt-in per pool
%{
  name: :ml_pool,
  zero_copy_enabled: true,  # NEW
  zero_copy_threshold_mb: 10  # Configurable
}
```

---

## Core Architecture

### Component Overview

```
┌──────────────────────────────────────────────────┐
│             Elixir Process                       │
│                                                  │
│  ┌──────────────────────────────────────┐       │
│  │  Snakepit.ZeroCopy.Manager           │       │
│  │  - Allocates shared memory regions   │       │
│  │  - Tracks region lifecycle           │       │
│  │  - Handles cleanup                   │       │
│  └──────────────┬───────────────────────┘       │
│                 │                                │
│  ┌──────────────▼───────────────────────┐       │
│  │  Snakepit.ZeroCopy.Region            │       │
│  │  - mmap() wrapper                    │       │
│  │  - Metadata (size, offset, format)   │       │
│  │  - Reference counting                │       │
│  └──────────────┬───────────────────────┘       │
└─────────────────┼────────────────────────────────┘
                  │
        ┌─────────▼──────────┐
        │  Shared Memory     │
        │  /dev/shm/         │
        │  (OS-level mmap)   │
        └─────────┬──────────┘
                  │
┌─────────────────┼────────────────────────────────┐
│                 │                                │
│  ┌──────────────▼───────────────────────┐       │
│  │  Python snakepit_bridge              │       │
│  │  - MemoryView wrapper                │       │
│  │  - NumPy zero-copy array             │       │
│  │  - Arrow IPC reader                  │       │
│  └──────────────────────────────────────┘       │
│                                                  │
│             Python Process                       │
└──────────────────────────────────────────────────┘
```

### Data Flow

#### 1. Elixir → Python (Large Tensor)

```elixir
# User code
tensor = generate_100mb_tensor()
Snakepit.execute("inference", %{model: "resnet", input: tensor})

# Internal flow:
1. ZeroCopy.Manager checks size: 100MB > 10MB threshold
2. Allocates shared memory region: /dev/shm/snakepit-XXXXX
3. Writes tensor data + metadata (shape, dtype) to region
4. Sends gRPC request with memory handle (not data!)
   Request: %{input_ref: "shm://snakepit-XXXXX"}
5. Python maps same memory region
6. Creates NumPy view (zero-copy): np.frombuffer(memview)
7. Processes tensor
8. Cleans up mapping
```

#### 2. Python → Elixir (Large Result)

```python
# Python adapter
result = model.predict(input)  # 50MB output tensor

# Internal flow:
1. Check size: 50MB > threshold
2. Write to shared memory via Arrow IPC
3. Return memory reference
   Response: %{output_ref: "shm://snakepit-YYYYY"}
4. Elixir maps memory
5. Deserializes Arrow to Elixir binary/tensor
6. Returns to user
7. Cleans up mapping
```

---

## Technical Implementation

### 1. Shared Memory Management (Elixir)

#### Module: `Snakepit.ZeroCopy.Manager`

```elixir
defmodule Snakepit.ZeroCopy.Manager do
  @moduledoc """
  Manages shared memory regions for zero-copy data transfer.

  Lifecycle:
  1. Allocate region when data exceeds threshold
  2. Write data + metadata
  3. Send reference to Python
  4. Wait for Python ACK
  5. Cleanup after use or timeout
  """

  use GenServer

  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end

  @doc """
  Allocate a shared memory region for data transfer.

  ## Options
  - size: bytes to allocate
  - format: :arrow | :raw | :numpy_pickle
  - timeout: cleanup timeout (default: 60s)
  """
  def allocate(size, opts \\ []) do
    GenServer.call(__MODULE__, {:allocate, size, opts})
  end

  @doc """
  Write data to a shared memory region.

  Automatically handles serialization based on data type.
  """
  def write(region_id, data, opts \\ []) do
    GenServer.call(__MODULE__, {:write, region_id, data, opts})
  end

  @doc """
  Read data from a shared memory region.

  Automatically handles deserialization based on metadata.
  """
  def read(region_id, opts \\ []) do
    GenServer.call(__MODULE__, {:read, region_id, opts})
  end

  @doc """
  Release a shared memory region.

  Called after Python acknowledges completion.
  """
  def release(region_id) do
    GenServer.cast(__MODULE__, {:release, region_id})
  end

  ## Implementation

  def init(opts) do
    base_path = opts[:base_path] || "/dev/shm"
    prefix = opts[:prefix] || "snakepit"
    max_regions = opts[:max_regions] || 1000

    state = %{
      base_path: base_path,
      prefix: prefix,
      max_regions: max_regions,
      regions: %{},  # region_id => %Region{}
      region_counter: 0
    }

    # Schedule periodic cleanup of stale regions
    :timer.send_interval(10_000, :cleanup_stale)

    {:ok, state}
  end

  def handle_call({:allocate, size, opts}, _from, state) do
    region_id = generate_region_id(state)
    path = build_path(state, region_id)

    case create_shm_file(path, size) do
      {:ok, fd} ->
        region = %Region{
          id: region_id,
          path: path,
          fd: fd,
          size: size,
          format: opts[:format] || :arrow,
          created_at: System.monotonic_time(:millisecond),
          timeout: opts[:timeout] || 60_000,
          ref_count: 1
        }

        new_state = put_in(state, [:regions, region_id], region)

        {:reply, {:ok, region_id, path}, new_state}

      {:error, reason} ->
        {:reply, {:error, reason}, state}
    end
  end

  def handle_call({:write, region_id, data, opts}, _from, state) do
    case Map.get(state.regions, region_id) do
      nil ->
        {:reply, {:error, :region_not_found}, state}

      region ->
        case serialize_and_write(region, data, opts) do
          :ok ->
            {:reply, :ok, state}

          {:error, reason} ->
            {:reply, {:error, reason}, state}
        end
    end
  end

  def handle_call({:read, region_id, opts}, _from, state) do
    case Map.get(state.regions, region_id) do
      nil ->
        {:reply, {:error, :region_not_found}, state}

      region ->
        case read_and_deserialize(region, opts) do
          {:ok, data} ->
            {:reply, {:ok, data}, state}

          {:error, reason} ->
            {:reply, {:error, reason}, state}
        end
    end
  end

  def handle_cast({:release, region_id}, state) do
    case Map.get(state.regions, region_id) do
      nil ->
        {:noreply, state}

      region ->
        cleanup_region(region)
        new_state = update_in(state, [:regions], &Map.delete(&1, region_id))
        {:noreply, new_state}
    end
  end

  def handle_info(:cleanup_stale, state) do
    now = System.monotonic_time(:millisecond)

    stale_regions =
      Enum.filter(state.regions, fn {_id, region} ->
        now - region.created_at > region.timeout
      end)

    Enum.each(stale_regions, fn {region_id, region} ->
      Logger.warning("Cleaning up stale region: #{region_id}")
      cleanup_region(region)
    end)

    new_state =
      update_in(state, [:regions], fn regions ->
        Map.drop(regions, Enum.map(stale_regions, fn {id, _} -> id end))
      end)

    {:noreply, new_state}
  end

  ## Private

  defp generate_region_id(state) do
    counter = state.region_counter + 1
    timestamp = System.system_time(:microsecond)
    "#{state.prefix}-#{timestamp}-#{counter}"
  end

  defp build_path(state, region_id) do
    Path.join(state.base_path, region_id)
  end

  defp create_shm_file(path, size) do
    # Use :file.open with :raw and :ram options for /dev/shm
    case :file.open(path, [:raw, :write, :binary, :exclusive]) do
      {:ok, fd} ->
        # Allocate space
        case :file.pwrite(fd, 0, :binary.copy(<<0>>, size)) do
          :ok -> {:ok, fd}
          error -> error
        end

      error -> error
    end
  end

  defp serialize_and_write(region, data, _opts) do
    case region.format do
      :arrow ->
        # Use Arrow IPC format for structured data
        serialize_arrow(region, data)

      :raw ->
        # Raw binary write
        :file.pwrite(region.fd, 0, data)

      :numpy_pickle ->
        # NumPy-compatible pickle format
        serialize_numpy(region, data)
    end
  end

  defp read_and_deserialize(region, _opts) do
    case :file.pread(region.fd, 0, region.size) do
      {:ok, data} ->
        deserialize_data(data, region.format)

      error -> error
    end
  end

  defp cleanup_region(region) do
    :file.close(region.fd)
    File.rm(region.path)
  end

  defp serialize_arrow(_region, _data) do
    # TODO: Implement Arrow IPC serialization
    # This will use :arrow library or NIF
    {:error, :not_implemented}
  end

  defp serialize_numpy(_region, _data) do
    # TODO: Implement NumPy-compatible format
    {:error, :not_implemented}
  end

  defp deserialize_data(_data, _format) do
    # TODO: Implement deserialization
    {:error, :not_implemented}
  end
end
```

#### Data Structure: `Region`

```elixir
defmodule Snakepit.ZeroCopy.Region do
  @moduledoc """
  Represents a shared memory region.
  """

  @type t :: %__MODULE__{
    id: String.t(),
    path: String.t(),
    fd: :file.fd(),
    size: non_neg_integer(),
    format: :arrow | :raw | :numpy_pickle,
    created_at: integer(),
    timeout: integer(),
    ref_count: non_neg_integer(),
    metadata: map()
  }

  defstruct [
    :id,
    :path,
    :fd,
    :size,
    :format,
    :created_at,
    :timeout,
    ref_count: 0,
    metadata: %{}
  ]
end
```

---

### 2. Protocol Extension (gRPC)

#### Extend `snakepit_bridge.proto`

```protobuf
message SharedMemoryRef {
  string region_id = 1;
  string path = 2;
  uint64 size = 3;
  string format = 4;  // "arrow", "raw", "numpy_pickle"
  map<string, string> metadata = 5;
}

message ExecuteToolRequest {
  string session_id = 1;
  string tool_name = 2;

  oneof parameters {
    string json_parameters = 3;
    bytes binary_parameters = 4;
    SharedMemoryRef shared_memory_ref = 5;  // NEW
  }
}

message ToolResponse {
  bool success = 1;
  string error = 2;

  oneof result {
    string json_result = 3;
    bytes binary_result = 4;
    SharedMemoryRef shared_memory_result = 5;  // NEW
  }
}
```

---

### 3. Python Side Implementation

#### Module: `snakepit_bridge/zero_copy.py`

```python
"""
Zero-copy memory management for Python side.
"""

import mmap
import os
import numpy as np
import pyarrow as pa
from typing import Optional, Union, Any
from pathlib import Path

class SharedMemoryView:
    """
    Python wrapper for shared memory regions.

    Provides zero-copy access to data written by Elixir.
    """

    def __init__(self, region_id: str, path: str, size: int, format: str):
        self.region_id = region_id
        self.path = Path(path)
        self.size = size
        self.format = format
        self._mmap = None
        self._data = None

    def __enter__(self):
        # Open and map the shared memory file
        fd = os.open(self.path, os.O_RDONLY)
        self._mmap = mmap.mmap(fd, self.size, access=mmap.ACCESS_READ)
        os.close(fd)  # Can close fd after mmap

        # Deserialize based on format
        if self.format == "arrow":
            self._data = self._read_arrow()
        elif self.format == "numpy_pickle":
            self._data = self._read_numpy()
        elif self.format == "raw":
            self._data = self._mmap
        else:
            raise ValueError(f"Unknown format: {self.format}")

        return self._data

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self._mmap:
            self._mmap.close()
        self._data = None

    def _read_arrow(self) -> Any:
        """
        Read Arrow IPC format from shared memory.

        Returns zero-copy NumPy array or Pandas DataFrame.
        """
        reader = pa.ipc.open_stream(self._mmap)
        table = reader.read_all()

        # Convert to NumPy (zero-copy if contiguous)
        if len(table.columns) == 1:
            return table.column(0).to_numpy(zero_copy_only=True)
        else:
            return table.to_pandas(zero_copy_only=True)

    def _read_numpy(self) -> np.ndarray:
        """
        Read NumPy array from pickle format.
        """
        import pickle
        return pickle.loads(self._mmap)

class SharedMemoryWriter:
    """
    Write data to shared memory for Elixir consumption.
    """

    def __init__(self, size: int, format: str = "arrow"):
        self.size = size
        self.format = format
        self.region_id = self._generate_id()
        self.path = Path(f"/dev/shm/{self.region_id}")
        self._mmap = None

    def __enter__(self):
        # Create shared memory file
        fd = os.open(
            self.path,
            os.O_CREAT | os.O_EXCL | os.O_RDWR,
            0o600
        )
        os.ftruncate(fd, self.size)
        self._mmap = mmap.mmap(fd, self.size, access=mmap.ACCESS_WRITE)
        os.close(fd)

        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self._mmap:
            self._mmap.flush()
            self._mmap.close()

    def write(self, data: Union[np.ndarray, Any]) -> dict:
        """
        Write data to shared memory.

        Returns metadata dict for gRPC response.
        """
        if self.format == "arrow":
            return self._write_arrow(data)
        elif self.format == "numpy_pickle":
            return self._write_numpy(data)
        elif self.format == "raw":
            self._mmap.write(data)
            return {"size": len(data)}
        else:
            raise ValueError(f"Unknown format: {self.format}")

    def _write_arrow(self, data: np.ndarray) -> dict:
        """
        Write NumPy array using Arrow IPC format.

        Zero-copy if possible.
        """
        # Convert to Arrow array (zero-copy if contiguous)
        arrow_array = pa.array(data, from_pandas=False)
        table = pa.Table.from_arrays([arrow_array], names=["data"])

        # Write to shared memory via IPC
        writer = pa.ipc.new_stream(self._mmap, table.schema)
        writer.write_table(table)
        writer.close()

        return {
            "shape": list(data.shape),
            "dtype": str(data.dtype),
            "size": data.nbytes
        }

    def _write_numpy(self, data: np.ndarray) -> dict:
        """
        Write NumPy array using pickle.
        """
        import pickle
        pickled = pickle.dumps(data, protocol=5)  # Protocol 5 for large data
        self._mmap.write(pickled)

        return {
            "shape": list(data.shape),
            "dtype": str(data.dtype),
            "size": len(pickled)
        }

    def get_reference(self) -> dict:
        """
        Get SharedMemoryRef for gRPC response.
        """
        return {
            "region_id": self.region_id,
            "path": str(self.path),
            "size": self.size,
            "format": self.format
        }

    @staticmethod
    def _generate_id() -> str:
        import time
        import random
        timestamp = int(time.time() * 1_000_000)
        rand = random.randint(0, 9999)
        return f"snakepit-py-{timestamp}-{rand}"
```

#### Integration with Adapter

```python
# In base adapter
from snakepit_bridge.zero_copy import SharedMemoryView, SharedMemoryWriter

class BaseAdapter:
    def __init__(self):
        self.zero_copy_threshold = 10 * 1024 * 1024  # 10MB
        self.zero_copy_enabled = True

    async def execute_tool(self, tool_name, parameters):
        # Check if parameters contain shared memory reference
        if isinstance(parameters, dict) and "shared_memory_ref" in parameters:
            ref = parameters["shared_memory_ref"]

            # Read from shared memory (zero-copy)
            with SharedMemoryView(**ref) as data:
                # data is now a NumPy array or similar
                result = await self._execute_with_data(tool_name, data)

            # Check if result is large
            if self._should_use_zero_copy(result):
                # Write to shared memory
                size = self._estimate_size(result)
                with SharedMemoryWriter(size, format="arrow") as writer:
                    metadata = writer.write(result)
                    return {
                        "shared_memory_result": writer.get_reference(),
                        "metadata": metadata
                    }
            else:
                # Small result, use regular serialization
                return {"json_result": self._serialize(result)}

        else:
            # Regular execution path
            return await self._execute_regular(tool_name, parameters)

    def _should_use_zero_copy(self, data) -> bool:
        """Check if data exceeds threshold for zero-copy."""
        if not self.zero_copy_enabled:
            return False

        size = self._estimate_size(data)
        return size >= self.zero_copy_threshold

    def _estimate_size(self, data) -> int:
        """Estimate data size in bytes."""
        if isinstance(data, np.ndarray):
            return data.nbytes
        elif isinstance(data, (list, dict)):
            import sys
            return sys.getsizeof(data)
        else:
            return 0
```

---

### 4. Integration with GRPCWorker

#### Extend `GRPCWorker.execute/4`

```elixir
def execute(worker_pid, command, args, timeout) do
  # Check if args contain large data
  case check_for_large_data(args) do
    {:large, data_key, data_value} ->
      # Allocate shared memory
      {:ok, region_id, path} =
        ZeroCopy.Manager.allocate(byte_size(data_value))

      # Write data
      :ok = ZeroCopy.Manager.write(region_id, data_value)

      # Replace data with reference
      args_with_ref = Map.put(args, data_key, %{
        shared_memory_ref: %{
          region_id: region_id,
          path: path,
          size: byte_size(data_value),
          format: "arrow"
        }
      })

      # Execute with reference
      result = GenServer.call(worker_pid,
        {:execute, command, args_with_ref, timeout},
        timeout + 1_000)

      # Cleanup
      ZeroCopy.Manager.release(region_id)

      # Check if result contains shared memory reference
      case result do
        {:ok, %{"shared_memory_result" => ref}} ->
          # Read from shared memory
          {:ok, data} = ZeroCopy.Manager.read(ref["region_id"])
          {:ok, data}

        other -> other
      end

    :small ->
      # Regular execution
      GenServer.call(worker_pid, {:execute, command, args, timeout},
                     timeout + 1_000)
  end
end

defp check_for_large_data(args) do
  threshold = 10 * 1024 * 1024  # 10MB

  Enum.find_value(args, :small, fn {key, value} ->
    if is_binary(value) and byte_size(value) > threshold do
      {:large, key, value}
    end
  end)
end
```

---

## Performance Characteristics

### Benchmarks (Estimated)

| Data Size | gRPC (v0.7.0) | Zero-Copy (v0.8.0) | Speedup |
|-----------|---------------|---------------------|---------|
| 1 MB      | 5ms           | 5ms                 | 1.0x    |
| 10 MB     | 50ms          | 8ms                 | 6.25x   |
| 100 MB    | 500ms         | 15ms                | 33x     |
| 1 GB      | 5s            | 100ms               | 50x     |

### Memory Usage

| Data Size | gRPC (v0.7.0) | Zero-Copy (v0.8.0) | Savings |
|-----------|---------------|---------------------|---------|
| 100 MB    | 300 MB        | 100 MB              | 67%     |
| 1 GB      | 3 GB          | 1 GB                | 67%     |

---

## Configuration API

### Pool Configuration

```elixir
config :snakepit,
  pools: [
    %{
      name: :ml_pool,
      worker_profile: :thread,  # v0.6.0
      python_environment: :uv,   # v0.7.0
      zero_copy_enabled: true,   # v0.8.0 NEW
      zero_copy_threshold_mb: 10,
      zero_copy_format: :arrow   # or :numpy_pickle, :raw
    }
  ]
```

### Runtime Toggle

```elixir
# Enable/disable per request
Snakepit.execute("inference", %{
  model: "resnet",
  input: large_tensor
}, zero_copy: true)

# Or globally
Snakepit.set_zero_copy_enabled(:ml_pool, true)
```

---

## Safety & Concurrency

### Thread Safety (v0.6.0 Foundation)

Zero-copy builds on v0.6.0 thread profile:

```
Thread Profile Workers (Python 3.13+)
  ↓ Multiple threads read same shared memory
Shared Memory Region (read-only after write)
  ↓ Memory fences ensure visibility
All threads see consistent data
```

### Cleanup Guarantees

1. **Worker crash**: Manager detects timeout, cleans up region
2. **Python crash**: Elixir owns memory, always cleaned up
3. **Elixir crash**: OS cleans /dev/shm on reboot
4. **Leak detection**: Periodic scan for orphaned regions

---

## Migration & Compatibility

### Zero Breaking Changes

```elixir
# v0.7.0 code works unchanged
Snakepit.execute("command", %{data: large_data})

# v0.8.0 automatically uses zero-copy if enabled
```

### Opt-In Adoption

```elixir
# Phase 1: Enable for one pool
pools: [
  %{name: :ml, zero_copy_enabled: true},
  %{name: :api, zero_copy_enabled: false}  # Still uses gRPC
]

# Phase 2: Tune threshold
pools: [
  %{name: :ml, zero_copy_threshold_mb: 5}  # More aggressive
]

# Phase 3: Enable globally
Application.put_env(:snakepit, :zero_copy_default, true)
```

---

## Testing Strategy

### Unit Tests

- Shared memory allocation/deallocation
- Serialization/deserialization formats
- Threshold detection
- Cleanup on timeout

### Integration Tests

- End-to-end data transfer
- Worker crash recovery
- Multiple concurrent transfers
- Large dataset handling (1GB+)

### Performance Tests

- Latency benchmarks (1MB, 10MB, 100MB, 1GB)
- Memory usage validation
- Throughput under load

---

## Summary

v0.8.0 Zero-Copy Data Transfer:

✅ **50x faster** for large data
✅ **67% less memory** usage
✅ **Transparent API** - automatic switching
✅ **Safe** - built on v0.6.0 thread foundations
✅ **Flexible** - configurable thresholds
✅ **Compatible** - zero breaking changes

**Value Proposition:**
Makes Snakepit the fastest Elixir↔Python bridge for ML/AI workloads involving large tensors, images, and datasets.

---

**Prerequisites:**
- v0.6.0 thread profile (concurrent access patterns)
- v0.7.0 environment management (Python 3.13+ recommended)

**Next Steps:**
See v0.8.0_implementation_plan.md for detailed build plan.
