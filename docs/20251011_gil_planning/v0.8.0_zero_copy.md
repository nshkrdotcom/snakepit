Based on the planning documents, "zero-copy" for v0.7.0 refers to **eliminating data serialization overhead between Elixir and Python** by sharing memory directly.

## Current v0.6.0 Architecture (With Copying)

Right now, data flows like this:

```
Elixir → Serialize to Protocol Buffers → gRPC → Deserialize in Python
Python → Serialize results → gRPC → Deserialize in Elixir
```

Each request involves:
- Converting Elixir data structures to Protobuf bytes
- Sending over gRPC (network-like protocol)
- Converting Protobuf bytes to Python objects
- Reverse process for responses

## Proposed v0.7.0 Zero-Copy Architecture

The vision is to **share memory directly** between BEAM and Python processes:

```
Elixir ← Direct memory access → Python
```

### Key Technologies Mentioned:

1. **Arrow/Parquet** - Columnar memory format both ecosystems understand
2. **Shared memory segments** - OS-level memory sharing (mmap)
3. **Memory-mapped files** - File-backed shared regions
4. **Direct binary references** - Pass pointers instead of copying data

### Benefits:

- **Massive performance gain** for large datasets (images, tensors, DataFrames)
- **Lower memory usage** - one copy instead of multiple
- **Reduced latency** - no serialization/deserialization overhead
- **Higher throughput** - bottleneck eliminated

### Example Use Case:

**Today (v0.6.0):**
```elixir
# 100MB image tensor
tensor_data = File.read!("image.bin")  # 100MB in Elixir
Snakepit.call("process_image", %{data: tensor_data})
# → Serialized to 100MB protobuf
# → Copied to Python (100MB)
# → Total: 300MB used, ~50ms overhead
```

**Future (v0.7.0 zero-copy):**
```elixir
# 100MB image tensor  
shared_ref = Snakepit.share_binary(tensor_data)  # Still 100MB
Snakepit.call("process_image", %{data_ref: shared_ref})
# → Only passes 8-byte pointer
# → Python reads directly from shared memory
# → Total: 100MB used, ~1ms overhead
```

This would make Snakepit **extremely competitive** for data-intensive workloads like ML inference, image processing, and scientific computing where you're moving large tensors/arrays between languages.

The planning doc positions it as a **natural evolution** after thread-mode proves the architecture can handle concurrent access patterns safely.



