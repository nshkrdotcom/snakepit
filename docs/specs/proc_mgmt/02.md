### **Architectural Document: A Resilient External Process Management System**

**Version:** 2.0
**Goal:** To create a fault-tolerant system that provides the strongest possible guarantees against orphaned Python processes under all conditions, including normal operation, graceful shutdown, and catastrophic VM failure.

#### 1. Core Principles

The architecture is built on three core principles:

1.  **Durability (The Ledger):** The single source of truth for all live external processes is a durable, on-disk log (the "PID Ledger"). This ledger must be updated transactionally and must survive the death of the BEAM.
2.  **Redundancy (The Reaper):** Cleanup is not a single event; it's a multi-layered process. We will have cleanup logic at application startup, during normal operation, and during shutdown.
3.  **Atomicity (Spawn and Record):** An external process is not considered "live" by the system until its OS PID has been successfully recorded in the durable ledger. This prevents race conditions where a process is spawned but the VM crashes before it can be tracked.

#### 2. Proposed Architecture

The system introduces a new central component, the **PID Ledger**, and modifies the lifecycle management of workers.



##### Component Deep Dive:

**A. The PID Ledger (New Component)**

This is the cornerstone of the new architecture. It is the durable, transactional record of every external Python process that *should* be alive.

*   **Implementation:** A `DETS` table. DETS (Disk Erlang Term Storage) is a perfect fit:
    *   It's built into OTP, requiring no external dependencies.
    *   It's disk-based and will survive a BEAM crash.
    *   It provides fast key-based lookups and atomic writes.
*   **Managed By:** A new GenServer, `Snakepit.Pool.PIDLedger`, will be the sole owner and writer to the DETS table to ensure consistency.
*   **Data Stored:** The ledger will store a record for each external process, keyed by the **OS PID**.
    *   `Key`: `OS_PID` (integer)
    *   `Value`: `%{worker_id: string(), elixir_pid: pid(), pgid: integer(), spawned_at: timestamp()}`
        *   `pgid` (Process Group ID) is critical for robust cleanup.

**B. The Application Starter (`snakepit/application.ex`)**

The application's startup sequence is modified to include a mandatory cleanup phase.

1.  **Start `PIDLedger`:** This process opens the DETS file.
2.  **Run Startup Reaper:** Before starting the `Pool` or any workers, the `PIDLedger` reads all PIDs from its DETS file.
3.  For each PID found, it forcefully terminates the entire process group (`kill -- -PGID`). This cleans up any orphans from a previous, ungraceful shutdown.
4.  Only after this cleanup completes does the rest of the application (Pool, WorkerSupervisor, etc.) start.

**C. The Worker Spawner (Refactored `GRPCWorker` or new `Spawner` module)**

The process of spawning a Python worker becomes a strictly controlled, multi-step transaction orchestrated by the `Worker.Starter`.

1.  **Execute `setsid`:** The Python server is no longer started directly with `Port.open`. Instead, it's spawned using a command like `setsid python grpc_server.py ...`. `setsid` is a standard Linux utility that runs a program in a new session, making it the leader of a new process group. The Process Group ID (PGID) will be the same as the process's PID.
2.  **Capture OS PID:** The spawner captures the OS PID of the new Python process.
3.  **Record to Ledger:** The spawner makes a synchronous call to the `PIDLedger` to durably record the new OS PID and its associated PGID.
4.  **Start OTP Worker:** Only after the PID is successfully persisted in the DETS file does the `WorkerSupervisor` start the `GRPCWorker` GenServer. The OS PID is passed as an argument to the `GRPCWorker`'s `init`. The worker then connects to the gRPC server at that PID.

**D. The Reaper (`snakepit/pool/application_cleanup.ex`)**

This module's role is expanded to be the definitive shutdown authority.

*   **On Graceful Shutdown (`terminate/2` called with `reason: :shutdown`):**
    1.  It gets the list of all live OS PIDs from the `PIDLedger`.
    2.  It sends `SIGTERM` to each process group (`kill -TERM -PGID`).
    3.  It waits for a grace period.
    4.  It re-checks the ledger and sends `SIGKILL` (`kill -KILL -PGID`) to any remaining process groups.
*   **On BEAM Crash (`terminate/2` called with other reason):** The `terminate` callback will still run and attempt a forceful `SIGKILL` on all registered process groups, providing a last line of defense.

---

#### 3. The Process Lifecycle (End-to-End)

This section details how the architecture handles every phase of a process's life.

##### **Phase 1: Application Startup**
1.  `Snakepit.Application` starts.
2.  `PIDLedger` starts and opens its DETS file.
3.  `PIDLedger` reads its contents. For a PID `12345` found in the file, it executes `kill -- -12345`.
4.  The rest of the application, including the `Pool`, starts in a clean environment.

##### **Phase 2: Spawning a New Worker**
1.  `Snakepit.Pool` needs a new worker and asks `WorkerSupervisor` to start one.
2.  `WorkerSupervisor` starts a `Worker.Starter`.
3.  The `Worker.Starter` executes the spawn command: `setsid python grpc_server.py --port 0`. It captures the OS PID, let's say `54321`.
4.  `Worker.Starter` calls `PIDLedger.register_process(54321, pgid: 54321, ...)`. The ledger writes this to DETS.
5.  After the ledger confirms the write, `Worker.Starter` starts the `GRPCWorker` GenServer, passing it the OS PID `54321`.
6.  The `GRPCWorker` connects to the gRPC server and begins operation.

##### **Phase 3: Worker Crash (OTP-level)**
1.  The Python process for worker `w1` (PID `54321`) crashes.
2.  The managing `GRPCWorker` GenServer crashes.
3.  The `Worker.Starter` supervisor for `w1` detects the crash and restarts the `GRPCWorker` (this is a flaw in the old model, we need to restart the OS process).
    *   **Correction:** The `Worker.Starter` restarts the *entire spawn sequence* (Phase 2). It first calls `PIDLedger.unregister_process(54321)` before spawning a new one.

##### **Phase 4: Graceful Application Shutdown**
1.  `mix app.stop` is called.
2.  Supervisors begin shutting down their children. `Snakepit.Pool` terminates.
3.  The `ApplicationCleanup` `terminate/2` callback is triggered.
4.  `ApplicationCleanup` gets all PIDs from the `PIDLedger` (e.g., `[54321, 54322]`).
5.  It executes `kill -TERM -54321` and `kill -TERM -54322`.
6.  It waits for 2 seconds.
7.  It checks if the processes are gone. If `54322` is still running, it executes `kill -KILL -54322`.
8.  It calls `PIDLedger.clear()` to empty the DETS file.

##### **Phase 5: Ungraceful VM Crash (`kill -9` the BEAM)**
1.  The BEAM VM is instantly terminated. No Elixir code runs.
2.  The Python processes (PIDs `54321`, `54322`) are now **orphaned**. Their parent is gone.
3.  The DETS file on disk still contains the records for PIDs `54321` and `54322`.
4.  **Recovery:** The application is restarted later. The lifecycle begins again at **Phase 1**, and the Startup Reaper finds and kills the orphaned processes `54321` and `54322`.

---

#### 4. Implementation Details & Code Snippets

**Spawning with `setsid` and Capturing PID (in a helper module):**

```elixir
# In a new Spawner module or inside Worker.Starter
def spawn_and_track(command, args) do
  full_cmd = "setsid #{command} #{Enum.join(args, " ")} & echo $!"
  
  case System.cmd("sh", ["-c", full_cmd]) do
    {output, 0} ->
      # Output will be the PID followed by a newline
      pid_str = String.trim(output)
      case Integer.parse(pid_str) do
        {pid, ""} -> {:ok, pid}
        _ -> {:error, "failed to parse pid from output: #{output}"}
      end
    {output, exit_code} ->
      {:error, "command failed with exit code #{exit_code}: #{output}"}
  end
end
```

**PID Ledger GenServer (Simplified):**

```elixir
defmodule Snakepit.Pool.PIDLedger do
  use GenServer

  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end

  def init(opts) do
    path = Keyword.fetch!(opts, :path)
    table = :dets.open_file(:pid_ledger, [{:file, path}])
    
    # Run startup cleanup
    cleanup_orphans(table)

    {:ok, %{table: table}}
  end

  def handle_call({:register, os_pid, data}, _from, state) do
    :ok = :dets.insert(state.table, {os_pid, data})
    {:reply, :ok, state}
  end
  
  def handle_call({:unregister, os_pid}, _from, state) do
    :ok = :dets.delete(state.table, os_pid)
    {:reply, :ok, state}
  end
  
  def handle_call(:get_all_pids, _from, state) do
    pids = :dets.select(state.table, [{{:"$1", :_}, [], [:"$1"]}])
    {:reply, pids, state}
  end

  defp cleanup_orphans(table) do
    Logger.info("PIDLedger: Starting orphan cleanup...")
    pids = :dets.select(table, [{{:"$1", :_}, [], [:"$1"]}])
    for pid <- pids do
      Logger.warning("Cleaning up potentially orphaned process group #{pid}")
      System.cmd("kill", ["-KILL", "-#{pid}"])
      :dets.delete(table, pid)
    end
    Logger.info("PIDLedger: Orphan cleanup complete.")
  end
end
```

This architecture provides multiple, overlapping layers of protection, ensuring that the system is resilient to crashes and that external resources are managed reliably, fulfilling the requirement for a truly robust solution.
