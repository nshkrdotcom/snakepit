# Phase 1: Snakepit Refinement - Detailed Technical Roadmap

**Duration:** 8-12 weeks  
**Goal:** Transform Snakepit from "works" to "production-ready polyglot bridge"

---

## Current State Analysis

Based on Document 1's feedback:

### ✅ **Strengths**
- BEAM-native orchestration (OTP supervision)
- Polyglot by design (not Python-first like Ray)
- Production hardening (DETS persistence, orphan cleanup)
- Bidirectional communication (gRPC streaming)

### ❌ **Weaknesses**
- Over-engineered application-level state (SessionStore, register_variable)
- ALTAR specification confusion (protocol vs. product)
- Complex/unclear API surface
- Insufficient documentation
- No Ray integration

---

## Phase 1 Roadmap: 4 Milestones

```
Week 1-2: Simplification (Remove ALTAR bloat)
Week 3-5: Core Refinement (Stable API)
Week 6-8: Ray Integration (Hybrid approach)
Week 9-12: Documentation & Examples (Production-ready)
```

---

## **Milestone 1: Simplification & API Clarity** (Weeks 1-2)

### **Goal:** Remove complexity, focus on core value proposition

### **Task 1.1: Remove Application-Level State Management**

**Problem:** Snakepit shouldn't manage state inside Python processes

**Current (Bad):**
```elixir
# Snakepit tries to manage Python state
Snakepit.execute("worker", %{
  action: :register_variable,
  var_name: "model",
  value: trained_model
})

# Later...
Snakepit.execute("worker", %{
  action: :get_variable,
  var_name: "model"
})
```

**Refactored (Good):**
```elixir
# Python process manages its own state
# Snakepit just routes messages

defmodule MyApp.MLService do
  def train_model(data) do
    # Python script handles state internally
    Snakepit.execute("ml_worker", %{
      method: "train",
      data: data
    })
  end

  def predict(input) do
    # Python script uses previously trained model
    Snakepit.execute("ml_worker", %{
      method: "predict", 
      input: input
    })
  end
end
```

**Implementation:**
```elixir
# Remove these modules:
# - lib/snakepit/session_store.ex (delete)
# - lib/snakepit/state_manager.ex (delete)

# Simplify worker to:
defmodule Snakepit.Worker do
  use GenServer

  # ONLY manages:
  # 1. Python process lifecycle
  # 2. Message routing
  # 3. Health checks

  def handle_call({:execute, request}, from, state) do
    # Send to Python via gRPC
    # Wait for response
    # Return to caller
    # That's it!
  end
end
```

**Acceptance Criteria:**
- [ ] Remove SessionStore completely
- [ ] Remove register_variable/get_variable API
- [ ] Update all tests to remove state management
- [ ] Python workers manage their own state (internal dicts, classes)

---

### **Task 1.2: Clarify ALTAR's Role (or Remove It)**

**Problem:** ALTAR is trying to be too many things

**Decision Point:** Choose ONE path:

#### **Option A: Remove ALTAR from Snakepit Core**
```elixir
# Snakepit becomes a pure process manager
# ALTAR becomes a separate library (altar_ex)

# Snakepit (core):
Snakepit.Pool.execute("worker", %{raw_python_call: "predict(x)"})

# ALTAR (optional addon):
ALTAR.Tool.call(:python_predictor, %{method: "predict", args: [x]})
# -> Under the hood, uses Snakepit
```

#### **Option B: Keep Minimal ALTAR Protocol**
```elixir
# ONLY standardize request/response format
# No Cost Manager, Governance Manager, Promotion Path

defmodule Snakepit.Protocol do
  @type request :: %{
    method: String.t(),
    args: list(),
    kwargs: map()
  }

  @type response :: %{
    status: :ok | :error,
    result: term(),
    error: String.t() | nil
  }
end
```

**Recommendation:** **Option A** - Remove ALTAR from Snakepit core

**Implementation:**
```bash
# Move ALTAR to separate repo
git subtree split --prefix=lib/snakepit/altar --branch altar-split
cd ../altar_ex
git init
git pull ../snakepit altar-split

# Update Snakepit to remove ALTAR references
# lib/snakepit/altar/* -> DELETE
```

**Acceptance Criteria:**
- [ ] ALTAR moved to separate repo (or deleted if not needed yet)
- [ ] Snakepit has no references to AESP, Cost Manager, Governance Manager
- [ ] Clean separation: Snakepit = process manager, ALTAR = protocol (separate concern)

---

### **Task 1.3: Simplify Public API**

**Current API (too many entry points):**
```elixir
Snakepit.execute/2
Snakepit.Pool.execute/3
Snakepit.Worker.Starter.start_worker/2
Snakepit.Client.call/2
Snakepit.Bridge.send_message/3
# ... too many!
```

**Refactored API (three clear entry points):**
```elixir
# 1. Pool-based execution (most common)
Snakepit.call(pool_name, request)

# 2. Direct worker execution (advanced)
Snakepit.Worker.call(worker_pid, request)

# 3. Supervised worker pools (setup)
Snakepit.Pool.start_link(name, script_path, opts)
```

**Implementation:**
```elixir
defmodule Snakepit do
  @moduledoc """
  Snakepit - Elixir to Python Process Bridge

  ## Quick Start

      # 1. Start a pool of Python workers
      {:ok, _} = Snakepit.Pool.start_link(
        :ml_workers,
        "path/to/worker.py",
        size: 5
      )

      # 2. Execute Python code
      {:ok, result} = Snakepit.call(:ml_workers, %{
        method: "predict",
        data: [1, 2, 3]
      })

  ## Architecture

  - Workers are supervised OTP processes
  - Communication via gRPC streaming
  - Automatic crash recovery via DETS persistence
  """

  @doc """
  Execute a request on a worker from the named pool.
  
  Blocks until response received or timeout.
  """
  @spec call(atom(), map(), keyword()) :: {:ok, term()} | {:error, term()}
  def call(pool_name, request, opts \\ []) do
    Snakepit.Pool.execute(pool_name, request, opts)
  end
end

defmodule Snakepit.Pool do
  @doc """
  Start a supervised pool of Python workers.
  """
  def start_link(name, script_path, opts \\ [])

  @doc """
  Execute request on next available worker.
  Uses round-robin selection.
  """
  def execute(pool_name, request, opts \\ [])

  @doc """
  Get pool statistics (active workers, queue depth, etc.)
  """
  def stats(pool_name)
end

defmodule Snakepit.Worker do
  @doc """
  Execute request on specific worker PID (advanced usage).
  """
  def call(worker_pid, request, opts \\ [])

  @doc """
  Check if worker is healthy.
  """
  def health_check(worker_pid)
end
```

**Acceptance Criteria:**
- [ ] Only 3 public modules: `Snakepit`, `Snakepit.Pool`, `Snakepit.Worker`
- [ ] All other modules are internal (prefixed with `Snakepit.Internal.*`)
- [ ] Clear module documentation with examples
- [ ] Deprecation warnings on old API

---

## **Milestone 2: Core Refinement** (Weeks 3-5)

### **Goal:** Bulletproof the fundamentals

### **Task 2.1: Improve gRPC Stability**

**Current Issues:**
- Connection drops not always detected
- Slow reconnection logic
- No circuit breaker pattern

**Implementation:**
```elixir
defmodule Snakepit.Internal.Connection do
  use GenServer

  @reconnect_backoff [100, 500, 1000, 2000, 5000]  # ms
  @health_check_interval 30_000  # 30 seconds

  def init(config) do
    state = %{
      channel: nil,
      status: :disconnected,
      reconnect_attempt: 0,
      last_health_check: nil
    }

    {:ok, state, {:continue, :connect}}
  end

  def handle_continue(:connect, state) do
    case GRPC.Stub.connect(state.config.endpoint) do
      {:ok, channel} ->
        schedule_health_check()
        {:noreply, %{state | channel: channel, status: :connected, reconnect_attempt: 0}}

      {:error, reason} ->
        backoff = Enum.at(@reconnect_backoff, state.reconnect_attempt, 5000)
        Logger.warn("Connection failed, retrying in #{backoff}ms: #{inspect(reason)}")
        Process.send_after(self(), :reconnect, backoff)
        {:noreply, %{state | reconnect_attempt: state.reconnect_attempt + 1}}
    end
  end

  def handle_info(:health_check, state) do
    case perform_health_check(state.channel) do
      :ok ->
        schedule_health_check()
        {:noreply, state}

      {:error, _reason} ->
        # Connection dead, reconnect
        {:noreply, %{state | status: :reconnecting}, {:continue, :connect}}
    end
  end

  defp schedule_health_check do
    Process.send_after(self(), :health_check, @health_check_interval)
  end
end
```

**Circuit Breaker:**
```elixir
defmodule Snakepit.Internal.CircuitBreaker do
  use GenServer

  # States: :closed (healthy), :open (failing), :half_open (testing)
  
  def init(_) do
    {:ok, %{
      state: :closed,
      failure_count: 0,
      last_failure: nil,
      failure_threshold: 5,
      timeout: 60_000  # 1 minute
    }}
  end

  def handle_call(:check, _from, %{state: :open} = state) do
    if elapsed_since(state.last_failure) > state.timeout do
      {:reply, :allow, %{state | state: :half_open}}
    else
      {:reply, {:error, :circuit_open}, state}
    end
  end

  def handle_call(:check, _from, state) do
    {:reply, :allow, state}
  end

  def handle_cast(:success, %{state: :half_open} = state) do
    {:noreply, %{state | state: :closed, failure_count: 0}}
  end

  def handle_cast(:failure, state) do
    new_count = state.failure_count + 1

    new_state =
      if new_count >= state.failure_threshold do
        %{state | state: :open, last_failure: System.monotonic_time(:millisecond)}
      else
        %{state | failure_count: new_count}
      end

    {:noreply, new_state}
  end
end
```

**Acceptance Criteria:**
- [ ] Automatic reconnection with exponential backoff
- [ ] Health checks every 30 seconds
- [ ] Circuit breaker prevents cascading failures
- [ ] Connection metrics emitted via Telemetry

---

### **Task 2.2: Enhance Process Pool Management**

**Current Issues:**
- Fixed pool size (no autoscaling)
- No queue management
- Workers can be starved

**Implementation:**
```elixir
defmodule Snakepit.Pool.Supervisor do
  use DynamicSupervisor

  def start_link(opts) do
    DynamicSupervisor.start_link(__MODULE__, opts, name: via_name(opts[:name]))
  end

  def init(opts) do
    config = %{
      name: opts[:name],
      script_path: opts[:script_path],
      min_workers: opts[:min_workers] || 2,
      max_workers: opts[:max_workers] || 10,
      target_queue_time_ms: opts[:target_queue_time_ms] || 100
    }

    # Start initial workers
    for _ <- 1..config.min_workers do
      start_worker(config)
    end

    # Monitor queue and scale
    schedule_scaling_check()

    DynamicSupervisor.init(strategy: :one_for_one, max_restarts: 10, max_seconds: 5)
  end

  defp schedule_scaling_check do
    Process.send_after(self(), :check_scaling, 5000)  # Every 5 seconds
  end

  def handle_info(:check_scaling, state) do
    stats = Snakepit.Pool.stats(state.config.name)

    cond do
      # Scale up if queue time is high
      stats.avg_queue_time_ms > state.config.target_queue_time_ms &&
      stats.worker_count < state.config.max_workers ->
        Logger.info("Scaling up pool #{state.config.name}")
        start_worker(state.config)

      # Scale down if workers are idle
      stats.idle_workers > 2 && stats.worker_count > state.config.min_workers ->
        Logger.info("Scaling down pool #{state.config.name}")
        stop_idle_worker(state.config)

      true ->
        :ok
    end

    schedule_scaling_check()
    {:noreply, state}
  end
end
```

**Queue Management:**
```elixir
defmodule Snakepit.Pool.Queue do
  use GenServer

  def init(opts) do
    {:ok, %{
      pending: :queue.new(),
      stats: %{
        enqueued: 0,
        processed: 0,
        avg_queue_time_ms: 0
      }
    }}
  end

  def handle_call({:enqueue, request, from}, _from, state) do
    item = {request, from, System.monotonic_time(:millisecond)}
    new_queue = :queue.in(item, state.pending)

    {:reply, :ok, %{state | pending: new_queue}}
  end

  def handle_call(:dequeue, _from, state) do
    case :queue.out(state.pending) do
      {{:value, {request, from, enqueued_at}}, new_queue} ->
        queue_time = System.monotonic_time(:millisecond) - enqueued_at

        # Update stats
        new_stats = update_stats(state.stats, queue_time)

        {:reply, {:ok, request, from, queue_time}, %{state | pending: new_queue, stats: new_stats}}

      {:empty, _} ->
        {:reply, :empty, state}
    end
  end

  defp update_stats(stats, queue_time) do
    # Exponential moving average
    alpha = 0.2
    new_avg = alpha * queue_time + (1 - alpha) * stats.avg_queue_time_ms

    %{stats |
      processed: stats.processed + 1,
      avg_queue_time_ms: new_avg
    }
  end
end
```

**Acceptance Criteria:**
- [ ] Autoscaling based on queue time (configurable)
- [ ] Min/max worker bounds
- [ ] Queue metrics (depth, avg wait time)
- [ ] Graceful worker shutdown (finish current task)

---

### **Task 2.3: Bidirectional Streaming Improvements**

**Current Issue:** Document mentions "Bidirectional Tool Bridge" but unclear how it works

**Implementation:**
```elixir
defmodule Snakepit.Stream do
  @moduledoc """
  Bidirectional streaming for long-running Python processes.

  Use cases:
  - Training ML models (stream progress updates)
  - Processing large datasets (stream results incrementally)
  - Interactive REPL (send commands, receive results)
  """

  def open(pool_name, request, callback) do
    # callback receives: {:data, chunk} | {:end, final_result} | {:error, reason}

    worker = Snakepit.Pool.checkout(pool_name)

    Snakepit.Worker.stream(worker, request, fn event ->
      case event do
        {:data, chunk} -> callback.({:data, chunk})
        {:end, result} -> callback.({:end, result})
        {:error, reason} -> callback.({:error, reason})
      end
    end)
  end
end

# Usage:
Snakepit.Stream.open(:ml_workers, %{method: "train", data: training_data}, fn event ->
  case event do
    {:data, %{epoch: epoch, loss: loss}} ->
      IO.puts("Epoch #{epoch}: loss = #{loss}")

    {:end, %{model: trained_model}} ->
      IO.puts("Training complete!")

    {:error, reason} ->
      IO.puts("Error: #{inspect(reason)}")
  end
end)
```

**Python Worker Side:**
```python
# worker.py
import grpc
import json

class Worker:
    def train(self, data, stream):
        """Long-running training with progress updates."""
        for epoch in range(100):
            loss = train_epoch(data)
            
            # Stream progress
            stream.send({
                "type": "progress",
                "data": {"epoch": epoch, "loss": loss}
            })
        
        # Stream final result
        stream.send({
            "type": "result",
            "data": {"model": serialize_model(self.model)}
        })
```

**Acceptance Criteria:**
- [ ] Bidirectional gRPC streaming works
- [ ] Python can send multiple chunks per request
- [ ] Elixir can cancel streams mid-flight
- [ ] Examples for: training progress, file processing, interactive REPL

---

## **Milestone 3: Ray Integration** (Weeks 6-8)

### **Goal:** Hybrid Elixir + Ray architecture (Path 2 from Document 1)

### **Task 3.1: Ray Adapter Design**

**Architecture:**
```
Elixir (Snakepit)
    ↓
Python Worker (Snakepit managed)
    ↓
Ray Cluster (worker submits jobs to Ray)
    ↓
1000s of Python Ray tasks
```

**Implementation:**
```elixir
defmodule Snakepit.Adapters.Ray do
  @moduledoc """
  Snakepit adapter for submitting jobs to Ray clusters.

  Architecture:
  1. Snakepit manages a Python "Ray proxy" worker
  2. Proxy worker submits jobs to Ray cluster
  3. Proxy streams results back to Elixir
  """

  def start_link(opts) do
    Snakepit.Pool.start_link(
      :ray_proxy,
      Path.join(:code.priv_dir(:snakepit), "python/ray_proxy.py"),
      size: 1,  # Only need one proxy
      ray_address: opts[:ray_address] || "auto"
    )
  end

  def submit_job(task, args, opts \\ []) do
    request = %{
      action: "submit_ray_job",
      task: task,
      args: args,
      num_replicas: opts[:num_replicas] || 100
    }

    Snakepit.call(:ray_proxy, request)
  end

  def get_job_status(job_id) do
    Snakepit.call(:ray_proxy, %{action: "get_job_status", job_id: job_id})
  end
end
```

**Python Ray Proxy:**
```python
# priv/python/ray_proxy.py
import ray
import json

ray.init(address="auto")  # Connect to Ray cluster

@ray.remote
def remote_task(func_name, args):
    """Generic Ray task wrapper."""
    # Import user function dynamically
    module = __import__(func_name.rsplit('.', 1)[0])
    func = getattr(module, func_name.rsplit('.', 1)[1])
    return func(*args)

class RayProxy:
    def submit_ray_job(self, task, args, num_replicas):
        """Submit job to Ray cluster."""
        # Create Ray tasks
        futures = [
            remote_task.remote(task, args)
            for _ in range(num_replicas)
        ]
        
        # Wait for completion
        results = ray.get(futures)
        
        return {
            "status": "complete",
            "results": results,
            "num_completed": len(results)
        }
    
    def get_job_status(self, job_id):
        """Get status of running Ray job."""
        # Implement using Ray's job API
        pass
```

**Acceptance Criteria:**
- [ ] Snakepit can submit jobs to Ray cluster
- [ ] Support for Ray remote functions and actors
- [ ] Job status tracking
- [ ] Examples: parallel data processing, ML training

---

### **Task 3.2: Hybrid Example: Elixir Phoenix + Snakepit + Ray**

**Use Case:** Phoenix app orchestrates massive ML workload via Ray

```elixir
defmodule MyAppWeb.MLController do
  use MyAppWeb, :controller

  def train_model(conn, %{"dataset" => dataset}) do
    # Submit training to Ray cluster
    {:ok, job_id} = Snakepit.Adapters.Ray.submit_job(
      "ml.train_model",
      [dataset],
      num_replicas: 1000  # 1000 Ray workers
    )

    # Poll for completion
    Task.async(fn -> poll_job(job_id) end)

    json(conn, %{job_id: job_id, status: "submitted"})
  end

  defp poll_job(job_id) do
    case Snakepit.Adapters.Ray.get_job_status(job_id) do
      {:ok, %{status: "complete", results: results}} ->
        # Store trained model
        save_model(results)

      {:ok, %{status: "running"}} ->
        Process.sleep(1000)
        poll_job(job_id)

      {:error, reason} ->
        Logger.error("Ray job failed: #{inspect(reason)}")
    end
  end
end
```

**Acceptance Criteria:**
- [ ] Full example Phoenix app using Snakepit + Ray
- [ ] Documentation: "When to use Ray vs. pure Snakepit"
- [ ] Benchmarks: Elixir Task.async vs. Snakepit pool vs. Ray cluster

---

## **Milestone 4: Documentation & Polish** (Weeks 9-12)

### **Goal:** Production-ready documentation and examples

### **Task 4.1: Comprehensive Documentation**

**Structure:**
```
docs/
├── 01_overview.md              # What is Snakepit?
├── 02_getting_started.md       # Quick start guide
├── 03_core_concepts.md         # Pools, workers, supervision
├── 04_python_worker_guide.md   # Writing Python workers
├── 05_advanced_patterns.md     # Streaming, timeouts, retries
├── 06_ray_integration.md       # Using Snakepit with Ray
├── 07_production_guide.md      # Deployment, monitoring
├── 08_api_reference.md         # Complete API docs
└── 09_troubleshooting.md       # Common issues

examples/
├── 01_basic_pool.exs
├── 02_ml_inference.exs
├── 03_streaming_results.exs
├── 04_ray_distributed.exs
├── 05_fastapi_integration/     # Full FastAPI + Snakepit example
│   ├── lib/my_app.ex
│   ├── python/api_worker.py
│   └── README.md
└── 06_django_integration/
```

**Key Documentation Sections:**

#### **02_getting_started.md**
```markdown
# Getting Started with Snakepit

## Installation

```elixir
def deps do
  [
    {:snakepit, "~> 1.0"}
  ]
end
```

## Your First Python Worker

```python
# worker.py
class SimpleWorker:
    def hello(self, name):
        return f"Hello, {name}!"
    
    def add(self, a, b):
        return a + b
```

```elixir
# Start pool
{:ok, _} = Snakepit.Pool.start_link(:simple, "worker.py", size: 3)

# Call Python
{:ok, result} = Snakepit.call(:simple, %{method: "hello", args: ["World"]})
IO.puts(result)  # => "Hello, World!"
```

## Next Steps
- [Core Concepts](03_core_concepts.md)
- [Python Worker Guide](04_python_worker_guide.md)
```

#### **06_ray_integration.md**
```markdown
# Ray Integration Guide

## When to Use Ray

Use Ray when you need:
- Thousands of parallel tasks (>100)
- Distributed across multiple machines
- ML training with massive datasets

Use Snakepit pools when you need:
- Dozens of parallel tasks (<100)
- Single machine is sufficient
- Lower latency (no network overhead)

## Architecture

```
Phoenix App (Elixir)
    ↓
Snakepit Pool (3-5 Python workers)
    ↓
Ray Cluster (1000+ Python workers)
```

## Example

[Full example code...]
```

**Acceptance Criteria:**
- [ ] All 9 documentation files complete
- [ ] 6+ runnable examples
- [ ] Inline code examples in every module
- [ ] Video walkthrough (15 minutes)

---

### **Task 4.2: Production Best Practices Guide**

**Topics:**
1. **Deployment**
   - Docker images for Elixir + Python
   - Kubernetes manifests
   - Systemd service files

2. **Monitoring**
   - Telemetry events
   - Prometheus metrics
   - Grafana dashboards

3. **Scaling**
   - When to scale pools vs. Ray
   - Autoscaling strategies
   - Resource limits

4. **Security**
   - Python sandboxing (optional)
   - Input validation
   - Rate limiting

**Implementation:**
```elixir
# Telemetry events
:telemetry.execute(
  [:snakepit, :pool, :execute, :start],
  %{system_time: System.system_time()},
  %{pool: pool_name}
)

:telemetry.execute(
  [:snakepit, :pool, :execute, :stop],
  %{duration: duration_ms},
  %{pool: pool_name, status: :ok}
)

# Prometheus metrics
defmodule Snakepit.Metrics do
  use PromEx, otp_app: :snakepit

  def metrics do
    [
      counter("snakepit.requests.total", "Total requests", [:pool, :status]),
      histogram("snakepit.request.duration", "Request duration", [:pool]),
      gauge("snakepit.pool.workers", "Active workers", [:pool]),
      gauge("snakepit.pool.queue_depth", "Queue depth", [:pool])
    ]
  end
end
```

**Acceptance Criteria:**
- [ ] Complete production guide
- [ ] Docker Compose example
- [ ] Kubernetes manifests
- [ ] Grafana dashboard JSON

---

### **Task 4.3: Test Suite & Benchmarks**

**Test Coverage Goals:**
- Core: 95%+
- Integrations: 80%+
- Examples: 100% (all run successfully)

**Benchmark Suite:**
```elixir
# benchmarks/pool_vs_ray.exs
Benchee.run(
  %{
    "Snakepit Pool (10 workers)" => fn ->
      Snakepit.call(:pool, %{method: "compute", args: [data]})
    end,
    
    "Snakepit + Ray (100 workers)" => fn ->
      Snakepit.Adapters.Ray.submit_job("compute", [data], num_replicas: 100)
    end,
    
    "Pure Elixir Task.async" => fn ->
      Task.async(fn -> compute_elixir(data) end) |> Task.await()
    end
  },
  time: 10,
  memory_time: 2
)
```

**Acceptance Criteria:**
- [ ] >90% test coverage
- [ ] Benchmarks for: pool sizes, Ray integration, streaming
- [ ] CI/CD pipeline (GitHub Actions)
- [ ] Automated releases

---

## **Deliverables Checklist**

### **Code**
- [ ] Simplified API (3 public modules)
- [ ] Removed ALTAR complexity
- [ ] Stable gRPC connection management
- [ ] Circuit breaker pattern
- [ ] Autoscaling pools
- [ ] Bidirectional streaming
- [ ] Ray adapter
- [ ] Telemetry events

### **Documentation**
- [ ] 9 documentation files
- [ ] 6+ runnable examples
- [ ] API reference (ExDoc)
- [ ] Production deployment guide
- [ ] Video walkthrough

### **Testing**
- [ ] 90%+ test coverage
- [ ] Integration tests (Ray)
- [ ] Benchmark suite
- [ ] CI/CD pipeline

### **Release**
- [ ] Snakepit 1.0.0 published to Hex
- [ ] GitHub release with changelog
- [ ] Announcement blog post
- [ ] HN/Reddit post

---

## **Success Metrics**

**Technical:**
- Pool startup time: <100ms
- Request latency (P99): <50ms
- Connection stability: 99.9% uptime
- Crash recovery: <1 second

**Adoption:**
- 100+ Hex downloads/week
- 50+ GitHub stars
- 5+ community PRs
- Featured in Elixir Radar

**Ecosystem:**
- Used by AI Research Framework
- Used by DataAdapter
- Used by FlowStone (Phase 4)
- Ray integration working

---

## **Risk Mitigation**

### **Risk 1: gRPC Complexity**
**Mitigation:** Use Mint + HTTP/2 as fallback if gRPC is problematic

### **Risk 2: Ray Changes API**
**Mitigation:** Pin Ray version, document compatibility matrix

### **Risk 3: Python GIL (pre-3.13)**
**Mitigation:** Document multiprocessing workaround, promote 3.13+ adoption

### **Risk 4: Scope Creep**
**Mitigation:** Strict "no new features" policy during refinement phase

---

## **Next Steps After Phase 1**

Once Snakepit 1.0 is released:

1. **Integrate into AI Research Framework** (Week 13-14)
   - Replace mock ML calls with real Snakepit workers
   - Benchmark ensemble performance

2. **Integrate into DataAdapter** (Week 15-16)
   - Python-based data transformations via Snakepit
   - Pandas/Polars support

3. **Begin FlowStone Development** (Week 17+)
   - Use Snakepit as foundational integration layer
   - FlowStone orchestrates Snakepit pools

---

## **Daily Workflow (Example Week)**

### **Week 1: Simplification**

**Monday:**
- [ ] Remove SessionStore module
- [ ] Update tests to remove state management
- [ ] Write migration guide for users

**Tuesday:**
- [ ] Remove ALTAR modules
- [ ] Create separate altar_ex repo (if keeping)
- [ ] Update README

**Wednesday:**
- [ ] Simplify public API to 3 modules
- [ ] Add deprecation warnings
- [ ] Update examples

**Thursday:**
- [ ] Write API documentation
- [ ] Create "upgrading from 0.x" guide
- [ ] Fix broken tests

**Friday:**
- [ ] Code review & refactoring
- [ ] Merge simplification PR
- [ ] Tag v0.9.0-rc.1 (release candidate)

---

**Ready to start?** I recommend beginning with **Task 1.1 (Remove Application-Level State)** as it has the highest impact and will clarify the rest of the architecture.

Would you like me to:
1. Generate specific Elixir code for any of these tasks?
2. Create a detailed Python worker specification?
3. Design the Ray integration protocol in more detail?






















# Phase 1: Snakepit Refinement - Detailed Technical Roadmap

**Duration:** 8-12 weeks  
**Goal:** Transform Snakepit from "works" to "production-ready polyglot bridge"

---

## Current State Analysis

Based on Document 1's feedback:

### ✅ **Strengths**
- BEAM-native orchestration (OTP supervision)
- Polyglot by design (not Python-first like Ray)
- Production hardening (DETS persistence, orphan cleanup)
- Bidirectional communication (gRPC streaming)

### ❌ **Weaknesses**
- Over-engineered application-level state (SessionStore, register_variable)
- ALTAR specification confusion (protocol vs. product)
- Complex/unclear API surface
- Insufficient documentation
- No Ray integration

---

## Phase 1 Roadmap: 4 Milestones

```
Week 1-2: Simplification (Remove ALTAR bloat)
Week 3-5: Core Refinement (Stable API)
Week 6-8: Ray Integration (Hybrid approach)
Week 9-12: Documentation & Examples (Production-ready)
```

---

## **Milestone 1: Simplification & API Clarity** (Weeks 1-2)

### **Goal:** Remove complexity, focus on core value proposition

### **Task 1.1: Remove Application-Level State Management**

**Problem:** Snakepit shouldn't manage state inside Python processes

**Current (Bad):**
```elixir
# Snakepit tries to manage Python state
Snakepit.execute("worker", %{
  action: :register_variable,
  var_name: "model",
  value: trained_model
})

# Later...
Snakepit.execute("worker", %{
  action: :get_variable,
  var_name: "model"
})
```

**Refactored (Good):**
```elixir
# Python process manages its own state
# Snakepit just routes messages

defmodule MyApp.MLService do
  def train_model(data) do
    # Python script handles state internally
    Snakepit.execute("ml_worker", %{
      method: "train",
      data: data
    })
  end

  def predict(input) do
    # Python script uses previously trained model
    Snakepit.execute("ml_worker", %{
      method: "predict", 
      input: input
    })
  end
end
```

**Implementation:**
```elixir
# Remove these modules:
# - lib/snakepit/session_store.ex (delete)
# - lib/snakepit/state_manager.ex (delete)

# Simplify worker to:
defmodule Snakepit.Worker do
  use GenServer

  # ONLY manages:
  # 1. Python process lifecycle
  # 2. Message routing
  # 3. Health checks

  def handle_call({:execute, request}, from, state) do
    # Send to Python via gRPC
    # Wait for response
    # Return to caller
    # That's it!
  end
end
```

**Acceptance Criteria:**
- [ ] Remove SessionStore completely
- [ ] Remove register_variable/get_variable API
- [ ] Update all tests to remove state management
- [ ] Python workers manage their own state (internal dicts, classes)

---

### **Task 1.2: Clarify ALTAR's Role (or Remove It)**

**Problem:** ALTAR is trying to be too many things

**Decision Point:** Choose ONE path:

#### **Option A: Remove ALTAR from Snakepit Core**
```elixir
# Snakepit becomes a pure process manager
# ALTAR becomes a separate library (altar_ex)

# Snakepit (core):
Snakepit.Pool.execute("worker", %{raw_python_call: "predict(x)"})

# ALTAR (optional addon):
ALTAR.Tool.call(:python_predictor, %{method: "predict", args: [x]})
# -> Under the hood, uses Snakepit
```

#### **Option B: Keep Minimal ALTAR Protocol**
```elixir
# ONLY standardize request/response format
# No Cost Manager, Governance Manager, Promotion Path

defmodule Snakepit.Protocol do
  @type request :: %{
    method: String.t(),
    args: list(),
    kwargs: map()
  }

  @type response :: %{
    status: :ok | :error,
    result: term(),
    error: String.t() | nil
  }
end
```

**Recommendation:** **Option A** - Remove ALTAR from Snakepit core

**Implementation:**
```bash
# Move ALTAR to separate repo
git subtree split --prefix=lib/snakepit/altar --branch altar-split
cd ../altar_ex
git init
git pull ../snakepit altar-split

# Update Snakepit to remove ALTAR references
# lib/snakepit/altar/* -> DELETE
```

**Acceptance Criteria:**
- [ ] ALTAR moved to separate repo (or deleted if not needed yet)
- [ ] Snakepit has no references to AESP, Cost Manager, Governance Manager
- [ ] Clean separation: Snakepit = process manager, ALTAR = protocol (separate concern)

---

### **Task 1.3: Simplify Public API**

**Current API (too many entry points):**
```elixir
Snakepit.execute/2
Snakepit.Pool.execute/3
Snakepit.Worker.Starter.start_worker/2
Snakepit.Client.call/2
Snakepit.Bridge.send_message/3
# ... too many!
```

**Refactored API (three clear entry points):**
```elixir
# 1. Pool-based execution (most common)
Snakepit.call(pool_name, request)

# 2. Direct worker execution (advanced)
Snakepit.Worker.call(worker_pid, request)

# 3. Supervised worker pools (setup)
Snakepit.Pool.start_link(name, script_path, opts)
```

**Implementation:**
```elixir
defmodule Snakepit do
  @moduledoc """
  Snakepit - Elixir to Python Process Bridge

  ## Quick Start

      # 1. Start a pool of Python workers
      {:ok, _} = Snakepit.Pool.start_link(
        :ml_workers,
        "path/to/worker.py",
        size: 5
      )

      # 2. Execute Python code
      {:ok, result} = Snakepit.call(:ml_workers, %{
        method: "predict",
        data: [1, 2, 3]
      })

  ## Architecture

  - Workers are supervised OTP processes
  - Communication via gRPC streaming
  - Automatic crash recovery via DETS persistence
  """

  @doc """
  Execute a request on a worker from the named pool.
  
  Blocks until response received or timeout.
  """
  @spec call(atom(), map(), keyword()) :: {:ok, term()} | {:error, term()}
  def call(pool_name, request, opts \\ []) do
    Snakepit.Pool.execute(pool_name, request, opts)
  end
end

defmodule Snakepit.Pool do
  @doc """
  Start a supervised pool of Python workers.
  """
  def start_link(name, script_path, opts \\ [])

  @doc """
  Execute request on next available worker.
  Uses round-robin selection.
  """
  def execute(pool_name, request, opts \\ [])

  @doc """
  Get pool statistics (active workers, queue depth, etc.)
  """
  def stats(pool_name)
end

defmodule Snakepit.Worker do
  @doc """
  Execute request on specific worker PID (advanced usage).
  """
  def call(worker_pid, request, opts \\ [])

  @doc """
  Check if worker is healthy.
  """
  def health_check(worker_pid)
end
```

**Acceptance Criteria:**
- [ ] Only 3 public modules: `Snakepit`, `Snakepit.Pool`, `Snakepit.Worker`
- [ ] All other modules are internal (prefixed with `Snakepit.Internal.*`)
- [ ] Clear module documentation with examples
- [ ] Deprecation warnings on old API

---

## **Milestone 2: Core Refinement** (Weeks 3-5)

### **Goal:** Bulletproof the fundamentals

### **Task 2.1: Improve gRPC Stability**

**Current Issues:**
- Connection drops not always detected
- Slow reconnection logic
- No circuit breaker pattern

**Implementation:**
```elixir
defmodule Snakepit.Internal.Connection do
  use GenServer

  @reconnect_backoff [100, 500, 1000, 2000, 5000]  # ms
  @health_check_interval 30_000  # 30 seconds

  def init(config) do
    state = %{
      channel: nil,
      status: :disconnected,
      reconnect_attempt: 0,
      last_health_check: nil
    }

    {:ok, state, {:continue, :connect}}
  end

  def handle_continue(:connect, state) do
    case GRPC.Stub.connect(state.config.endpoint) do
      {:ok, channel} ->
        schedule_health_check()
        {:noreply, %{state | channel: channel, status: :connected, reconnect_attempt: 0}}

      {:error, reason} ->
        backoff = Enum.at(@reconnect_backoff, state.reconnect_attempt, 5000)
        Logger.warn("Connection failed, retrying in #{backoff}ms: #{inspect(reason)}")
        Process.send_after(self(), :reconnect, backoff)
        {:noreply, %{state | reconnect_attempt: state.reconnect_attempt + 1}}
    end
  end

  def handle_info(:health_check, state) do
    case perform_health_check(state.channel) do
      :ok ->
        schedule_health_check()
        {:noreply, state}

      {:error, _reason} ->
        # Connection dead, reconnect
        {:noreply, %{state | status: :reconnecting}, {:continue, :connect}}
    end
  end

  defp schedule_health_check do
    Process.send_after(self(), :health_check, @health_check_interval)
  end
end
```

**Circuit Breaker:**
```elixir
defmodule Snakepit.Internal.CircuitBreaker do
  use GenServer

  # States: :closed (healthy), :open (failing), :half_open (testing)
  
  def init(_) do
    {:ok, %{
      state: :closed,
      failure_count: 0,
      last_failure: nil,
      failure_threshold: 5,
      timeout: 60_000  # 1 minute
    }}
  end

  def handle_call(:check, _from, %{state: :open} = state) do
    if elapsed_since(state.last_failure) > state.timeout do
      {:reply, :allow, %{state | state: :half_open}}
    else
      {:reply, {:error, :circuit_open}, state}
    end
  end

  def handle_call(:check, _from, state) do
    {:reply, :allow, state}
  end

  def handle_cast(:success, %{state: :half_open} = state) do
    {:noreply, %{state | state: :closed, failure_count: 0}}
  end

  def handle_cast(:failure, state) do
    new_count = state.failure_count + 1

    new_state =
      if new_count >= state.failure_threshold do
        %{state | state: :open, last_failure: System.monotonic_time(:millisecond)}
      else
        %{state | failure_count: new_count}
      end

    {:noreply, new_state}
  end
end
```

**Acceptance Criteria:**
- [ ] Automatic reconnection with exponential backoff
- [ ] Health checks every 30 seconds
- [ ] Circuit breaker prevents cascading failures
- [ ] Connection metrics emitted via Telemetry

---

### **Task 2.2: Enhance Process Pool Management**

**Current Issues:**
- Fixed pool size (no autoscaling)
- No queue management
- Workers can be starved

**Implementation:**
```elixir
defmodule Snakepit.Pool.Supervisor do
  use DynamicSupervisor

  def start_link(opts) do
    DynamicSupervisor.start_link(__MODULE__, opts, name: via_name(opts[:name]))
  end

  def init(opts) do
    config = %{
      name: opts[:name],
      script_path: opts[:script_path],
      min_workers: opts[:min_workers] || 2,
      max_workers: opts[:max_workers] || 10,
      target_queue_time_ms: opts[:target_queue_time_ms] || 100
    }

    # Start initial workers
    for _ <- 1..config.min_workers do
      start_worker(config)
    end

    # Monitor queue and scale
    schedule_scaling_check()

    DynamicSupervisor.init(strategy: :one_for_one, max_restarts: 10, max_seconds: 5)
  end

  defp schedule_scaling_check do
    Process.send_after(self(), :check_scaling, 5000)  # Every 5 seconds
  end

  def handle_info(:check_scaling, state) do
    stats = Snakepit.Pool.stats(state.config.name)

    cond do
      # Scale up if queue time is high
      stats.avg_queue_time_ms > state.config.target_queue_time_ms &&
      stats.worker_count < state.config.max_workers ->
        Logger.info("Scaling up pool #{state.config.name}")
        start_worker(state.config)

      # Scale down if workers are idle
      stats.idle_workers > 2 && stats.worker_count > state.config.min_workers ->
        Logger.info("Scaling down pool #{state.config.name}")
        stop_idle_worker(state.config)

      true ->
        :ok
    end

    schedule_scaling_check()
    {:noreply, state}
  end
end
```

**Queue Management:**
```elixir
defmodule Snakepit.Pool.Queue do
  use GenServer

  def init(opts) do
    {:ok, %{
      pending: :queue.new(),
      stats: %{
        enqueued: 0,
        processed: 0,
        avg_queue_time_ms: 0
      }
    }}
  end

  def handle_call({:enqueue, request, from}, _from, state) do
    item = {request, from, System.monotonic_time(:millisecond)}
    new_queue = :queue.in(item, state.pending)

    {:reply, :ok, %{state | pending: new_queue}}
  end

  def handle_call(:dequeue, _from, state) do
    case :queue.out(state.pending) do
      {{:value, {request, from, enqueued_at}}, new_queue} ->
        queue_time = System.monotonic_time(:millisecond) - enqueued_at

        # Update stats
        new_stats = update_stats(state.stats, queue_time)

        {:reply, {:ok, request, from, queue_time}, %{state | pending: new_queue, stats: new_stats}}

      {:empty, _} ->
        {:reply, :empty, state}
    end
  end

  defp update_stats(stats, queue_time) do
    # Exponential moving average
    alpha = 0.2
    new_avg = alpha * queue_time + (1 - alpha) * stats.avg_queue_time_ms

    %{stats |
      processed: stats.processed + 1,
      avg_queue_time_ms: new_avg
    }
  end
end
```

**Acceptance Criteria:**
- [ ] Autoscaling based on queue time (configurable)
- [ ] Min/max worker bounds
- [ ] Queue metrics (depth, avg wait time)
- [ ] Graceful worker shutdown (finish current task)

---

### **Task 2.3: Bidirectional Streaming Improvements**

**Current Issue:** Document mentions "Bidirectional Tool Bridge" but unclear how it works

**Implementation:**
```elixir
defmodule Snakepit.Stream do
  @moduledoc """
  Bidirectional streaming for long-running Python processes.

  Use cases:
  - Training ML models (stream progress updates)
  - Processing large datasets (stream results incrementally)
  - Interactive REPL (send commands, receive results)
  """

  def open(pool_name, request, callback) do
    # callback receives: {:data, chunk} | {:end, final_result} | {:error, reason}

    worker = Snakepit.Pool.checkout(pool_name)

    Snakepit.Worker.stream(worker, request, fn event ->
      case event do
        {:data, chunk} -> callback.({:data, chunk})
        {:end, result} -> callback.({:end, result})
        {:error, reason} -> callback.({:error, reason})
      end
    end)
  end
end

# Usage:
Snakepit.Stream.open(:ml_workers, %{method: "train", data: training_data}, fn event ->
  case event do
    {:data, %{epoch: epoch, loss: loss}} ->
      IO.puts("Epoch #{epoch}: loss = #{loss}")

    {:end, %{model: trained_model}} ->
      IO.puts("Training complete!")

    {:error, reason} ->
      IO.puts("Error: #{inspect(reason)}")
  end
end)
```

**Python Worker Side:**
```python
# worker.py
import grpc
import json

class Worker:
    def train(self, data, stream):
        """Long-running training with progress updates."""
        for epoch in range(100):
            loss = train_epoch(data)
            
            # Stream progress
            stream.send({
                "type": "progress",
                "data": {"epoch": epoch, "loss": loss}
            })
        
        # Stream final result
        stream.send({
            "type": "result",
            "data": {"model": serialize_model(self.model)}
        })
```

**Acceptance Criteria:**
- [ ] Bidirectional gRPC streaming works
- [ ] Python can send multiple chunks per request
- [ ] Elixir can cancel streams mid-flight
- [ ] Examples for: training progress, file processing, interactive REPL

---

## **Milestone 3: Ray Integration** (Weeks 6-8)

### **Goal:** Hybrid Elixir + Ray architecture (Path 2 from Document 1)

### **Task 3.1: Ray Adapter Design**

**Architecture:**
```
Elixir (Snakepit)
    ↓
Python Worker (Snakepit managed)
    ↓
Ray Cluster (worker submits jobs to Ray)
    ↓
1000s of Python Ray tasks
```

**Implementation:**
```elixir
defmodule Snakepit.Adapters.Ray do
  @moduledoc """
  Snakepit adapter for submitting jobs to Ray clusters.

  Architecture:
  1. Snakepit manages a Python "Ray proxy" worker
  2. Proxy worker submits jobs to Ray cluster
  3. Proxy streams results back to Elixir
  """

  def start_link(opts) do
    Snakepit.Pool.start_link(
      :ray_proxy,
      Path.join(:code.priv_dir(:snakepit), "python/ray_proxy.py"),
      size: 1,  # Only need one proxy
      ray_address: opts[:ray_address] || "auto"
    )
  end

  def submit_job(task, args, opts \\ []) do
    request = %{
      action: "submit_ray_job",
      task: task,
      args: args,
      num_replicas: opts[:num_replicas] || 100
    }

    Snakepit.call(:ray_proxy, request)
  end

  def get_job_status(job_id) do
    Snakepit.call(:ray_proxy, %{action: "get_job_status", job_id: job_id})
  end
end
```

**Python Ray Proxy:**
```python
# priv/python/ray_proxy.py
import ray
import json

ray.init(address="auto")  # Connect to Ray cluster

@ray.remote
def remote_task(func_name, args):
    """Generic Ray task wrapper."""
    # Import user function dynamically
    module = __import__(func_name.rsplit('.', 1)[0])
    func = getattr(module, func_name.rsplit('.', 1)[1])
    return func(*args)

class RayProxy:
    def submit_ray_job(self, task, args, num_replicas):
        """Submit job to Ray cluster."""
        # Create Ray tasks
        futures = [
            remote_task.remote(task, args)
            for _ in range(num_replicas)
        ]
        
        # Wait for completion
        results = ray.get(futures)
        
        return {
            "status": "complete",
            "results": results,
            "num_completed": len(results)
        }
    
    def get_job_status(self, job_id):
        """Get status of running Ray job."""
        # Implement using Ray's job API
        pass
```

**Acceptance Criteria:**
- [ ] Snakepit can submit jobs to Ray cluster
- [ ] Support for Ray remote functions and actors
- [ ] Job status tracking
- [ ] Examples: parallel data processing, ML training

---

### **Task 3.2: Hybrid Example: Elixir Phoenix + Snakepit + Ray**

**Use Case:** Phoenix app orchestrates massive ML workload via Ray

```elixir
defmodule MyAppWeb.MLController do
  use MyAppWeb, :controller

  def train_model(conn, %{"dataset" => dataset}) do
    # Submit training to Ray cluster
    {:ok, job_id} = Snakepit.Adapters.Ray.submit_job(
      "ml.train_model",
      [dataset],
      num_replicas: 1000  # 1000 Ray workers
    )

    # Poll for completion
    Task.async(fn -> poll_job(job_id) end)

    json(conn, %{job_id: job_id, status: "submitted"})
  end

  defp poll_job(job_id) do
    case Snakepit.Adapters.Ray.get_job_status(job_id) do
      {:ok, %{status: "complete", results: results}} ->
        # Store trained model
        save_model(results)

      {:ok, %{status: "running"}} ->
        Process.sleep(1000)
        poll_job(job_id)

      {:error, reason} ->
        Logger.error("Ray job failed: #{inspect(reason)}")
    end
  end
end
```

**Acceptance Criteria:**
- [ ] Full example Phoenix app using Snakepit + Ray
- [ ] Documentation: "When to use Ray vs. pure Snakepit"
- [ ] Benchmarks: Elixir Task.async vs. Snakepit pool vs. Ray cluster

---

## **Milestone 4: Documentation & Polish** (Weeks 9-12)

### **Goal:** Production-ready documentation and examples

### **Task 4.1: Comprehensive Documentation**

**Structure:**
```
docs/
├── 01_overview.md              # What is Snakepit?
├── 02_getting_started.md       # Quick start guide
├── 03_core_concepts.md         # Pools, workers, supervision
├── 04_python_worker_guide.md   # Writing Python workers
├── 05_advanced_patterns.md     # Streaming, timeouts, retries
├── 06_ray_integration.md       # Using Snakepit with Ray
├── 07_production_guide.md      # Deployment, monitoring
├── 08_api_reference.md         # Complete API docs
└── 09_troubleshooting.md       # Common issues

examples/
├── 01_basic_pool.exs
├── 02_ml_inference.exs
├── 03_streaming_results.exs
├── 04_ray_distributed.exs
├── 05_fastapi_integration/     # Full FastAPI + Snakepit example
│   ├── lib/my_app.ex
│   ├── python/api_worker.py
│   └── README.md
└── 06_django_integration/
```

**Key Documentation Sections:**

#### **02_getting_started.md**
```markdown
# Getting Started with Snakepit

## Installation

```elixir
def deps do
  [
    {:snakepit, "~> 1.0"}
  ]
end
```

## Your First Python Worker

```python
# worker.py
class SimpleWorker:
    def hello(self, name):
        return f"Hello, {name}!"
    
    def add(self, a, b):
        return a + b
```

```elixir
# Start pool
{:ok, _} = Snakepit.Pool.start_link(:simple, "worker.py", size: 3)

# Call Python
{:ok, result} = Snakepit.call(:simple, %{method: "hello", args: ["World"]})
IO.puts(result)  # => "Hello, World!"
```

## Next Steps
- [Core Concepts](03_core_concepts.md)
- [Python Worker Guide](04_python_worker_guide.md)
```

#### **06_ray_integration.md**
```markdown
# Ray Integration Guide

## When to Use Ray

Use Ray when you need:
- Thousands of parallel tasks (>100)
- Distributed across multiple machines
- ML training with massive datasets

Use Snakepit pools when you need:
- Dozens of parallel tasks (<100)
- Single machine is sufficient
- Lower latency (no network overhead)

## Architecture

```
Phoenix App (Elixir)
    ↓
Snakepit Pool (3-5 Python workers)
    ↓
Ray Cluster (1000+ Python workers)
```

## Example

[Full example code...]
```

**Acceptance Criteria:**
- [ ] All 9 documentation files complete
- [ ] 6+ runnable examples
- [ ] Inline code examples in every module
- [ ] Video walkthrough (15 minutes)

---

### **Task 4.2: Production Best Practices Guide**

**Topics:**
1. **Deployment**
   - Docker images for Elixir + Python
   - Kubernetes manifests
   - Systemd service files

2. **Monitoring**
   - Telemetry events
   - Prometheus metrics
   - Grafana dashboards

3. **Scaling**
   - When to scale pools vs. Ray
   - Autoscaling strategies
   - Resource limits

4. **Security**
   - Python sandboxing (optional)
   - Input validation
   - Rate limiting

**Implementation:**
```elixir
# Telemetry events
:telemetry.execute(
  [:snakepit, :pool, :execute, :start],
  %{system_time: System.system_time()},
  %{pool: pool_name}
)

:telemetry.execute(
  [:snakepit, :pool, :execute, :stop],
  %{duration: duration_ms},
  %{pool: pool_name, status: :ok}
)

# Prometheus metrics
defmodule Snakepit.Metrics do
  use PromEx, otp_app: :snakepit

  def metrics do
    [
      counter("snakepit.requests.total", "Total requests", [:pool, :status]),
      histogram("snakepit.request.duration", "Request duration", [:pool]),
      gauge("snakepit.pool.workers", "Active workers", [:pool]),
      gauge("snakepit.pool.queue_depth", "Queue depth", [:pool])
    ]
  end
end
```

**Acceptance Criteria:**
- [ ] Complete production guide
- [ ] Docker Compose example
- [ ] Kubernetes manifests
- [ ] Grafana dashboard JSON

---

### **Task 4.3: Test Suite & Benchmarks**

**Test Coverage Goals:**
- Core: 95%+
- Integrations: 80%+
- Examples: 100% (all run successfully)

**Benchmark Suite:**
```elixir
# benchmarks/pool_vs_ray.exs
Benchee.run(
  %{
    "Snakepit Pool (10 workers)" => fn ->
      Snakepit.call(:pool, %{method: "compute", args: [data]})
    end,
    
    "Snakepit + Ray (100 workers)" => fn ->
      Snakepit.Adapters.Ray.submit_job("compute", [data], num_replicas: 100)
    end,
    
    "Pure Elixir Task.async" => fn ->
      Task.async(fn -> compute_elixir(data) end) |> Task.await()
    end
  },
  time: 10,
  memory_time: 2
)
```

**Acceptance Criteria:**
- [ ] >90% test coverage
- [ ] Benchmarks for: pool sizes, Ray integration, streaming
- [ ] CI/CD pipeline (GitHub Actions)
- [ ] Automated releases

---

## **Deliverables Checklist**

### **Code**
- [ ] Simplified API (3 public modules)
- [ ] Removed ALTAR complexity
- [ ] Stable gRPC connection management
- [ ] Circuit breaker pattern
- [ ] Autoscaling pools
- [ ] Bidirectional streaming
- [ ] Ray adapter
- [ ] Telemetry events

### **Documentation**
- [ ] 9 documentation files
- [ ] 6+ runnable examples
- [ ] API reference (ExDoc)
- [ ] Production deployment guide
- [ ] Video walkthrough

### **Testing**
- [ ] 90%+ test coverage
- [ ] Integration tests (Ray)
- [ ] Benchmark suite
- [ ] CI/CD pipeline

### **Release**
- [ ] Snakepit 1.0.0 published to Hex
- [ ] GitHub release with changelog
- [ ] Announcement blog post
- [ ] HN/Reddit post

---

## **Success Metrics**

**Technical:**
- Pool startup time: <100ms
- Request latency (P99): <50ms
- Connection stability: 99.9% uptime
- Crash recovery: <1 second

**Adoption:**
- 100+ Hex downloads/week
- 50+ GitHub stars
- 5+ community PRs
- Featured in Elixir Radar

**Ecosystem:**
- Used by AI Research Framework
- Used by DataAdapter
- Used by FlowStone (Phase 4)
- Ray integration working

---

## **Risk Mitigation**

### **Risk 1: gRPC Complexity**
**Mitigation:** Use Mint + HTTP/2 as fallback if gRPC is problematic

### **Risk 2: Ray Changes API**
**Mitigation:** Pin Ray version, document compatibility matrix

### **Risk 3: Python GIL (pre-3.13)**
**Mitigation:** Document multiprocessing workaround, promote 3.13+ adoption

### **Risk 4: Scope Creep**
**Mitigation:** Strict "no new features" policy during refinement phase

---

## **Next Steps After Phase 1**

Once Snakepit 1.0 is released:

1. **Integrate into AI Research Framework** (Week 13-14)
   - Replace mock ML calls with real Snakepit workers
   - Benchmark ensemble performance

2. **Integrate into DataAdapter** (Week 15-16)
   - Python-based data transformations via Snakepit
   - Pandas/Polars support

3. **Begin FlowStone Development** (Week 17+)
   - Use Snakepit as foundational integration layer
   - FlowStone orchestrates Snakepit pools

---

## **Daily Workflow (Example Week)**

### **Week 1: Simplification**

**Monday:**
- [ ] Remove SessionStore module
- [ ] Update tests to remove state management
- [ ] Write migration guide for users

**Tuesday:**
- [ ] Remove ALTAR modules
- [ ] Create separate altar_ex repo (if keeping)
- [ ] Update README

**Wednesday:**
- [ ] Simplify public API to 3 modules
- [ ] Add deprecation warnings
- [ ] Update examples

**Thursday:**
- [ ] Write API documentation
- [ ] Create "upgrading from 0.x" guide
- [ ] Fix broken tests

**Friday:**
- [ ] Code review & refactoring
- [ ] Merge simplification PR
- [ ] Tag v0.9.0-rc.1 (release candidate)

---

**Ready to start?** I recommend beginning with **Task 1.1 (Remove Application-Level State)** as it has the highest impact and will clarify the rest of the architecture.

Would you like me to:
1. Generate specific Elixir code for any of these tasks?
2. Create a detailed Python worker specification?
3. Design the Ray integration protocol in more detail?


















Let me analyze the **actual problem** with Snakepit's session management by looking at the use case and implementation strategy.

## The Core Question: Is SessionStore Necessary?

### **Your Original Intent (Valid Reasoning)**

You likely built SessionStore because:

```python
# Python ML workflow - needs stateful session
model = load_pretrained_model()  # Expensive! (5 seconds)

# Later, many inference calls
for request in requests:
    prediction = model.predict(request)  # Fast (50ms)
```

**Problem:** If you restart the Python worker for each request, you reload the model **every time** (5 seconds each).

**Your Solution:** SessionStore tracks which Python workers have which models loaded.

```elixir
# First call: load model
Snakepit.execute("ml_worker", %{
  action: :load_model,
  model_name: "gpt2"
})

# Subsequent calls: reuse loaded model
Snakepit.execute("ml_worker", %{
  action: :predict,
  model_name: "gpt2",  # SessionStore routes to worker with this model
  input: "Hello"
})
```

**This makes sense!** You're avoiding expensive reloads.

---

## The Problem: **How You Implemented It**

The issue isn't that session management is wrong—it's that **Elixir shouldn't manage Python's internal state**.

### **What You Probably Built (Overengineered)**

```elixir
defmodule Snakepit.SessionStore do
  # Elixir tracks what's inside each Python worker
  def register_variable(worker_pid, var_name, value) do
    # Store in ETS: {worker_pid, var_name, value}
  end

  def get_variable(worker_pid, var_name) do
    # Lookup from ETS
  end

  def worker_has_variable?(worker_pid, var_name) do
    # Check ETS
  end
end

# Usage (BAD - Elixir managing Python state)
Snakepit.execute("worker", %{action: :store_variable, var: "model", value: model_blob})
# ^ Elixir now "knows" worker has "model" variable

Snakepit.execute("worker", %{action: :get_variable, var: "model"})
# ^ Elixir looks up variable and tells Python to fetch it
```

**Why this is overengineered:**

1. **State duplication**: Both Elixir (ETS) and Python (process memory) track state
2. **Serialization overhead**: Passing `value` through gRPC
3. **Complex routing**: Elixir decides which worker to use based on what's loaded
4. **Brittleness**: If Python worker crashes, Elixir's ETS is stale

---

## The Right Way: **Python Manages Its Own State**

### **Simple Pattern: Worker-Local State**

```python
# worker.py - Python manages state internally
class MLWorker:
    def __init__(self):
        self.models = {}  # Python dict, NOT tracked by Elixir
    
    def load_model(self, model_name):
        """Load model and store in self.models"""
        if model_name not in self.models:
            self.models[model_name] = load_pretrained_model(model_name)
        return {"status": "loaded", "model": model_name}
    
    def predict(self, model_name, input_text):
        """Use previously loaded model"""
        if model_name not in self.models:
            # Auto-load if not present
            self.load_model(model_name)
        
        model = self.models[model_name]
        return {"prediction": model.predict(input_text)}
```

```elixir
# Elixir side - MUCH SIMPLER
# No SessionStore needed!

# First call: Python loads model (slow)
{:ok, _} = Snakepit.call(:ml_worker, %{
  method: "load_model",
  args: ["gpt2"]
})

# Subsequent calls: Python reuses loaded model (fast)
{:ok, prediction} = Snakepit.call(:ml_worker, %{
  method: "predict",
  args: ["gpt2", "Hello world"]
})
```

**Key insight:** The Python worker is **long-lived** (supervised by Elixir), so it naturally maintains state between calls. **No ETS needed.**

---

## When SessionStore **Is** Justified

### **Scenario 1: Load Balancing with Affinity**

If you have **multiple workers** and want to route requests to the worker that already has the model loaded:

```elixir
defmodule Snakepit.Pool.Affinity do
  @moduledoc """
  Route requests to workers with specific state loaded.
  
  Example: Worker 1 has model "gpt2", Worker 2 has model "bert"
  Requests for "gpt2" should go to Worker 1.
  """

  def execute(pool_name, request) do
    model_name = request[:model_name]
    
    # Check which worker has this model loaded
    case find_worker_with_model(pool_name, model_name) do
      {:ok, worker_pid} ->
        # Route to specific worker
        Snakepit.Worker.call(worker_pid, request)
      
      :not_found ->
        # Pick any available worker, it will load the model
        worker_pid = Snakepit.Pool.checkout(pool_name)
        Snakepit.Worker.call(worker_pid, request)
    end
  end

  defp find_worker_with_model(pool_name, model_name) do
    # Look up in ETS: which worker has this model?
    # This is where SessionStore would be useful
  end
end
```

**This is a valid use case** for tracking state in Elixir, BUT:

1. **Python should tell Elixir** what it has loaded (not vice versa)
2. **Use it for routing only**, not for managing Python's state

### **Better Implementation: Health Check Reports State**

```python
# worker.py
class MLWorker:
    def __init__(self):
        self.models = {}  # Python owns this
    
    def health_check(self):
        """Report current state to Elixir"""
        return {
            "status": "healthy",
            "loaded_models": list(self.models.keys()),
            "memory_mb": get_memory_usage()
        }
```

```elixir
defmodule Snakepit.Pool.StateTracker do
  use GenServer

  # Periodically poll workers for their state
  def init(pool_name) do
    schedule_health_check()
    {:ok, %{pool_name: pool_name, worker_states: %{}}}
  end

  def handle_info(:health_check, state) do
    # Ask each worker what it has loaded
    workers = Snakepit.Pool.list_workers(state.pool_name)
    
    new_states =
      Enum.reduce(workers, %{}, fn worker_pid, acc ->
        case Snakepit.Worker.call(worker_pid, %{method: "health_check"}) do
          {:ok, health} ->
            Map.put(acc, worker_pid, health)
          _ ->
            acc
        end
      end)

    schedule_health_check()
    {:noreply, %{state | worker_states: new_states}}
  end

  def find_worker_with_model(pool_name, model_name) do
    state = :sys.get_state(via_name(pool_name))
    
    Enum.find_value(state.worker_states, fn {worker_pid, health} ->
      if model_name in health["loaded_models"] do
        {:ok, worker_pid}
      end
    end) || :not_found
  end
end
```

**Key differences from your SessionStore:**

1. ✅ **Pull-based**: Elixir polls Python for state (not push)
2. ✅ **Python is source of truth**: Health check returns actual state
3. ✅ **Eventual consistency**: If worker crashes, next health check detects it
4. ✅ **Optional optimization**: Only used for routing, not required

---

## Scenario 2: Distributed Workers Across Nodes

If workers run on **different Erlang nodes**, you might want a **global registry**:

```elixir
# Worker on node1@localhost has model "gpt2"
# Worker on node2@localhost has model "bert"

defmodule Snakepit.GlobalRegistry do
  use Horde.Registry

  def register_worker(worker_pid, capabilities) do
    Horde.Registry.register(__MODULE__, {:worker, capabilities}, worker_pid)
  end

  def find_worker_with_capability(capability) do
    case Horde.Registry.lookup(__MODULE__, {:worker, capability}) do
      [{worker_pid, _}] -> {:ok, worker_pid}
      [] -> :not_found
    end
  end
end
```

**But even here:** Python still owns its state, Horde just tracks **which node** has **which capabilities**.

---

## The Real Problem: **ALTAR Confusion**

Looking at your documents, the bigger issue is **ALTAR trying to be too many things**:

### **What ALTAR Seems To Be**

```elixir
# ALTAR as "Tool Protocol"
ALTAR.execute(%{
  tool: :python_ml_service,
  method: "predict",
  args: [data],
  
  # These are overengineered:
  cost_budget: 100,  # Cost Manager?
  governance_policy: :require_approval,  # Governance Manager?
  promotion_path: :local_to_grid  # LATER -> GRID promotion?
})
```

**Problems:**

1. **Cost Manager**: This belongs in application logic, not the bridge
2. **Governance Manager**: This belongs in application logic, not the bridge
3. **Promotion Path**: "No modification" local→distributed is a lie (Document 1 is correct)

### **What ALTAR Should Be (If Anything)**

Just a **standardized request/response format**:

```elixir
defmodule ALTAR.Protocol do
  @type request :: %{
    method: String.t(),        # Required: function to call
    args: list(),              # Required: positional args
    kwargs: map(),             # Optional: keyword args
    timeout_ms: pos_integer()  # Optional: timeout
  }

  @type response :: %{
    status: :ok | :error,
    result: term(),
    error: String.t() | nil,
    duration_ms: pos_integer()
  }
end
```

**That's it.** Anything more belongs in a separate library.

---

## Refactoring Strategy

### **Step 1: Keep Session Management, Simplify It**

```elixir
defmodule Snakepit.Pool.Affinity do
  @moduledoc """
  OPTIONAL module for routing requests to workers with specific state.
  
  Only use if you have:
  - Multiple workers per pool
  - Expensive-to-load state (models, databases)
  - High request volume (>100/sec)
  """

  def execute(pool_name, request, opts \\ []) do
    if opts[:use_affinity] do
      execute_with_affinity(pool_name, request)
    else
      # Simple round-robin (default)
      Snakepit.Pool.execute(pool_name, request)
    end
  end

  defp execute_with_affinity(pool_name, request) do
    # Try to find worker with required state
    case find_best_worker(pool_name, request) do
      {:ok, worker_pid} ->
        Snakepit.Worker.call(worker_pid, request)
      
      :not_found ->
        # Fallback to any worker
        Snakepit.Pool.execute(pool_name, request)
    end
  end

  defp find_best_worker(pool_name, request) do
    # Query worker states (from periodic health checks)
    # Return worker that already has required state loaded
  end
end
```

**Key changes:**

1. Make affinity **opt-in** (not default)
2. Python workers report state via health checks
3. Only use for optimization, not correctness

### **Step 2: Remove ALTAR Complexity**

```elixir
# REMOVE these modules entirely:
# - lib/snakepit/altar/cost_manager.ex
# - lib/snakepit/altar/governance_manager.ex
# - lib/snakepit/altar/promotion_path.ex

# KEEP (if useful):
# - lib/snakepit/altar/protocol.ex (just request/response format)

# OR: Move ALTAR to separate package
# {:altar_ex, "~> 1.0"}  # Optional addon
```

### **Step 3: Simplify Public API**

```elixir
# Three core functions, that's it:

# 1. Pool execution (most common)
Snakepit.call(pool_name, request, opts \\ [])

# 2. Direct worker execution (advanced)
Snakepit.Worker.call(worker_pid, request, opts \\ [])

# 3. Streaming (long-running tasks)
Snakepit.stream(pool_name, request, callback)
```

---

## Final Verdict

### **Session Management: Keep or Remove?**

**Keep** if you need:
- Multiple workers per pool
- Expensive-to-load state (models, databases, connections)
- Request routing based on worker state

**Remove** if:
- Single worker per pool
- Stateless Python functions
- You're adding complexity without measurable benefit

### **Recommended Approach**

1. **Keep affinity/routing** as an **optional module** (`Snakepit.Pool.Affinity`)
2. **Make it opt-in** via config: `use_affinity: true`
3. **Python owns state**, Elixir just tracks it for routing
4. **Health checks** are the source of truth
5. **Remove ALTAR** complexity (Cost Manager, Governance, Promotion)

### **The Real Overengineering**

It's not SessionStore itself—it's:

1. **ALTAR trying to do too much** (cost, governance, promotion)
2. **Elixir trying to manage Python's state** (instead of just routing)
3. **Complex APIs** (too many entry points)

---

## Code Example: Simplified Affinity

```elixir
defmodule Snakepit.Pool do
  # Default: simple round-robin (no state tracking)
  def call(pool_name, request) do
    worker_pid = checkout_worker(pool_name)
    Snakepit.Worker.call(worker_pid, request)
  end

  # Opt-in: affinity-based routing
  def call_with_affinity(pool_name, request, affinity_key) do
    case Snakepit.Pool.Affinity.find_worker(pool_name, affinity_key) do
      {:ok, worker_pid} ->
        Snakepit.Worker.call(worker_pid, request)
      
      :not_found ->
        # Fallback to normal routing
        call(pool_name, request)
    end
  end
end

# Usage:
# Simple (no state tracking)
Snakepit.Pool.call(:ml_workers, %{method: "predict", args: [data]})

# With affinity (tracks which worker has which model loaded)
Snakepit.Pool.call_with_affinity(
  :ml_workers,
  %{method: "predict", model: "gpt2", args: [data]},
  affinity_key: "gpt2"  # Route to worker with gpt2 loaded
)
```

**Does this clarify the overengineering?** The concept of session management isn't wrong—it's the **implementation strategy** and **scope creep** (ALTAR) that made it complex.
