### The Core Principles of the Guardian Architecture

1.  **The Elixir Application is the Sole Owner:** The BEAM VM is the undisputed parent and owner of all Python processes. It is responsible for their birth, death, and cleanup under all circumstances.
2.  **Persistence is Non-Negotiable:** The state of external processes (their PIDs) must survive BEAM restarts. If the Elixir app crashes and restarts, its first job is to hunt down and deal with any children it left behind.
3.  **OS-Level Grouping is Key:** We will not manage individual PIDs. We will manage **OS process groups**. This is the single most powerful technique. When you kill a process group, you kill the process and *all of its descendants*, preventing shell wrappers or sub-sub-processes from being orphaned.
4.  **A Layered Shutdown Strategy:** Always attempt a graceful shutdown first (SIGTERM), but have an infallible, forceful finalizer (SIGKILL).

---

### The Architectural Components

We will build this system using three main components, refining what you already have.

#### 1. The Process Registry: A Durable, Transactional Store

Your current `ProcessRegistry` uses ETS, which is volatile. It disappears when the BEAM stops. We will replace this with a disk-based, crash-safe store.

*   **Technology Choice:** **DETS (Disk-Based ETS)**. It's part of OTP, provides an almost identical API to ETS, and is designed for exactly this purpose. It's much safer than writing to a plain text file, as it handles file corruption and partial writes.

*   **What to Store:** We need more than just the PID. Each entry in the DETS table should be a tuple like:
    `{worker_id, %{os_pid: integer(), pgid: integer(), command: string(), started_at: datetime(), beam_run_id: term()}}`

    *   `worker_id`: Your internal Elixir-side ID.
    *   `os_pid`: The OS PID of the Python process.
    *   **`pgid` (Process Group ID):** The most important field. We will operate on this.
    *   `command`: The full command used to start the process, for forensics.
    *   `started_at`: Timestamp for identifying long-running/stale processes.
    *   **`beam_run_id`:** A unique identifier generated *every time the Elixir application starts*. This is the secret weapon for cleaning up after a crash.

#### 2. The Worker Spawner: OS-Aware Process Creation

This is where we harden the `GRPCWorker`'s `init` callback.

*   **Creating Process Groups:** When you spawn the Python process, you must ensure it runs in its own process group. On Linux/macOS, the `setsid` command is the perfect tool.

    Instead of spawning `python3 ...`, you will spawn `setsid python3 ...`. `setsid` runs the command in a new session, making it the leader of a new process group. The PGID will be the same as the PID of the spawned process.

*   **The Spawning Workflow in `GRPCWorker.init`:**
    1.  Construct the full Python command: `["python3", "grpc_server.py", "--port", "0"]`.
    2.  Prepend the `setsid` command: `["setsid" | python_command]`.
    3.  Open the `Port` using this new command.
    4.  **Crucially, get the OS PID immediately.** This PID is also the PGID.
    5.  **Immediately and atomically, write the full record (including the `pgid` and the current `beam_run_id`) to the DETS table.**
    6.  Only after the record is safely persisted on disk do you proceed with connecting, monitoring, etc.

#### 3. The Application Guardian: Startup and Shutdown Sentinel

This component is a `GenServer` that starts with your application. It has two critical responsibilities.

*   **On Application Startup (`init/1`):**
    1.  Generate a new, unique `beam_run_id` for this run of the application (e.g., using `:erlang.unique_integer()`). Store it in the `GenServer`'s state.
    2.  Open the DETS table.
    3.  **Perform the Orphan Hunt:** Iterate through every single record in the DETS table.
    4.  For each record, compare its `beam_run_id` with the current run's ID.
    5.  **If they do not match, it's a potential orphan from a previous, crashed run.**
    6.  For each potential orphan, verify if the process group is still alive using its `pgid` (e.g., `System.cmd("kill", ["-0", "-#{pgid}"])`).
    7.  If it is alive, it is a confirmed orphan. **Terminate it forcefully** with `kill -KILL -<pgid>`. There is no need for grace; its parent is dead.
    8.  Remove the stale record from the DETS table.
    9.  After the hunt is complete, the system is in a clean state, ready to start new workers.

*   **On Application Shutdown (`terminate/2`):**
    1.  This will be similar to your existing `ApplicationCleanup`, but more robust.
    2.  It will iterate through the DETS table for all processes belonging to the *current* `beam_run_id`.
    3.  For each process, it will execute the **Layered Shutdown Strategy**:
        *   **Graceful:** Send `SIGTERM` to the entire process group: `System.cmd("kill", ["-TERM", "-#{pgid}"])`.
        *   **Wait:** Wait for a short, defined period (e.g., 2-5 seconds).
        *   **Forceful:** If the process group is still alive, send `SIGKILL`: `System.cmd("kill", ["-KILL", "-#{pgid}"])`.
    4.  This ensures that every process started by this run of the application is terminated, one way or another.

---

### Putting It All Together: The Workflow

**1. Application Start:**
*   `Guardian.start_link()` is called by your main supervisor.
*   `Guardian.init/1` generates a `run_id`.
*   It scans the DETS table, finds stale records from old runs, and kills any corresponding lingering OS process groups. The DETS table is now clean.

**2. Spawning a Worker (`GRPCWorker.init`):**
*   The `Pool` asks the `WorkerSupervisor` to start a child.
*   `GRPCWorker.init` is called.
*   It spawns `setsid python3 ...`.
*   It gets the PID (which is also the PGID).
*   It writes `{worker_id, %{pgid: pgid, beam_run_id: current_run_id, ...}}` to the DETS table.
*   It proceeds with its normal logic (waiting for `GRPC_READY`, connecting, etc.).

**3. Normal Worker Termination (`GRPCWorker.terminate`):**
*   The worker is shut down normally (e.g., by the pool).
*   `GRPCWorker.terminate/2` is called.
*   It sends `SIGTERM` to the process group (`-#{pgid}`).
*   It waits a moment.
*   It confirms the process group is dead.
*   It removes its entry from the DETS table.

**4. Worker Crash:**
*   The `Port` monitor fires a `:DOWN` message.
*   `GRPCWorker.handle_info` receives it and stops.
*   `GRPCWorker.terminate/2` is called. Because the reason is not `:normal`, it should immediately send `SIGKILL` to the process group to be safe.
*   It removes the entry from the DETS table.
*   The `Worker.Starter` supervisor restarts the `GRPCWorker`, and the cycle begins anew.

**5. Application Shutdown (or Crash):**
*   The BEAM begins its shutdown sequence.
*   `Guardian.terminate/2` is called.
*   It reads all entries from DETS with the *current* `beam_run_id`.
*   It iterates through them, issuing `SIGTERM` then `SIGKILL` to each process group.
*   *Crucially, if the BEAM itself crashes hard*, the DETS file is left as-is. The next time the application starts, the **Orphan Hunt** will find and eliminate these processes.

### Code Sketch

```elixir
# lib/snakepit/guardian.ex
defmodule Snakepit.Guardian do
  use GenServer

  @dets_table :snakepit_process_registry
  @dets_file "/var/tmp/snakepit_registry.dets" # Or a better path

  def start_link(_opts) do
    GenServer.start_link(__MODULE__, [], name: __MODULE__)
  end

  # Public API for workers
  def register_process(worker_id, pgid, command, beam_run_id) do
    record = %{pgid: pgid, command: command, started_at: :os.timestamp(), beam_run_id: beam_run_id}
    :dets.insert(@dets_table, {worker_id, record})
  end

  def unregister_process(worker_id) do
    :dets.delete(@dets_table, worker_id)
  end

  def get_current_run_id() do
    GenServer.call(__MODULE__, :get_run_id)
  end

  @impl true
  def init(_opts) do
    # 1. Open/create the DETS table
    :ok = :dets.open_file(@dets_table, type: :set, file: @dets_file)

    # 2. Generate unique ID for this application run
    beam_run_id = :erlang.unique_integer([:positive, :monotonic])

    # 3. Perform the Orphan Hunt
    hunt_for_orphans(beam_run_id)

    {:ok, %{beam_run_id: beam_run_id}}
  end

  @impl true
  def terminate(_reason, state) do
    # Perform graceful, then forceful shutdown of all managed processes for this run
    all_records = :dets.match_object(@dets_table, {:_, %{beam_run_id: state.beam_run_id}})
    for {worker_id, %{pgid: pgid}} <- all_records do
      Logger.info("Shutting down worker #{worker_id} with PGID #{pgid}")
      System.cmd("kill", ["-TERM", "-#{pgid}"])
    end

    # Give them time to die
    Process.sleep(2000)

    for {worker_id, %{pgid: pgid}} <- all_records do
      if is_process_group_alive?(pgid) do
        Logger.warning("PGID #{pgid} for worker #{worker_id} did not terminate gracefully. Sending SIGKILL.")
        System.cmd("kill", ["-KILL", "-#{pgid}"])
      end
    end
    :dets.close(@dets_table)
  end
  
  # ... handle_call for :get_run_id ...

  defp hunt_for_orphans(current_run_id) do
    stale_records = :dets.select(@dets_table, [{{:_, :"$1"}, [{:"/=", {elem, :"$1", 4}, current_run_id}], [:"$_"]}])

    for {worker_id, %{pgid: pgid}} <- stale_records do
      if is_process_group_alive?(pgid) do
        Logger.warning("Found orphaned process group #{pgid} for worker #{worker_id} from previous run. Terminating forcefully.")
        System.cmd("kill", ["-KILL", "-#{pgid}"])
      end
      :dets.delete(@dets_table, worker_id)
    end
  end

  defp is_process_group_alive?(pgid) do
    # "kill -0" checks for existence without sending a signal
    {_, exit_code} = System.cmd("kill", ["-0", "-#{pgid}"], stderr_to_stdout: true)
    exit_code == 0
  end
end

# In GRPCWorker.init/1
def init(opts) do
  # ...
  beam_run_id = Snakepit.Guardian.get_current_run_id()
  command = ["setsid", adapter.executable_path() | script_with_args]
  port = Port.open({:spawn_executable, hd(command)}, args: tl(command), ...)
  {:ok, os_pid} = Port.info(port, :os_pid) # This is the PGID
  
  :ok = Snakepit.Guardian.register_process(worker_id, os_pid, Enum.join(command, " "), beam_run_id)
  # ...
end
```

This architecture provides the transactional, persistent guarantees you're looking for and is the standard for building truly robust systems of this nature. It moves the problem from "hoping processes die" to "actively ensuring processes die."
