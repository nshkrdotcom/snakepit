### **1. The Core Problem: The OTP/OS Impedance Mismatch**

The fundamental issue is that an external Python process lives in the OS world, while your application lives in the OTP world. OTP's supervisors are masters of managing OTP processes, but they are blind to OS PIDs. When the link between them breaks unexpectedly, you get an orphan.

**Common Failure Modes Leading to Orphans:**

1.  **BEAM VM Crash**: The entire Elixir VM is killed instantly (e.g., `kill -9 beam.smp`, kernel OOM killer). No Elixir code runs, no `terminate` callbacks are called, and the Python process is left running.
2.  **Un-trapped Exits**: A bug causes the worker GenServer to crash in a way that its `terminate` callback is skipped.
3.  **Deploy Issues**: During a deploy, the old version of the app might be killed uncleanly, leaving its Python workers running while the new version starts its own.
4.  **Hardware Failure/Reboot**: The machine reboots, and on startup, old stale processes from a previous run might still exist if their PIDs were recycled.

The "Lifecycle Guardian" architecture is designed to solve all of these cases.

### **2. The "Lifecycle Guardian" Architecture**

The central principle is: **Every external OS process must be owned by a single, supervised Elixir process. The owner's sole responsibility is to ensure the external process's lifecycle matches its own.**

This architecture has four essential components:

1.  **The Guardian Process**: A dedicated GenServer for each Python worker.
2.  **The Persistent PID Registry**: A durable, on-disk log of all active external PIDs.
3.  **The Startup Reaper**: A process that runs *once* at application boot to kill orphans from a previous, crashed run.
4.  **The Shutdown Enforcer**: A final-resort cleanup hook in the main application `terminate` callback.

```
+---------------------------------+
|      Snakepit Application       |
|  (lib/snakepit/application.ex)  |
+---------------------------------+
           | supervises
           v
+---------------------------------+
|       Main Supervisor           |
+---------------------------------+
  | supervises     | supervises
  v                v
+----------------+----------------+      +---------------------------+
| Startup Reaper |  Process Registry  |----->|  /tmp/snakepit_pids.log   | (On-disk PID file)
| (Runs once)    |  (GenServer + ETS) |      +---------------------------+
+----------------+----------------+
  | supervises
  v
+---------------------------------+
|    Pool & Worker Supervisor     |
+---------------------------------+
           | supervises N
           v
+---------------------------------+      +---------------------------+
|   Worker Guardian (GenServer)   |----->|   python grpc_server.py   | (OS Process)
| (Owns one Python process)       |      | (PID: 12345)              |
+---------------------------------+      +---------------------------+
```

---

#### **Component 1: The Worker Guardian (`Snakepit.Worker.Guardian`)**

This is the most critical component. It replaces the current `GRPCWorker`. Instead of the worker being both the connection manager *and* the process starter, we separate those concerns. The Guardian's only job is to manage the OS process.

**Responsibilities:**

*   **`init/1`**:
    1.  Starts the `grpc_server.py` process using `Port.open`. This is vital because a `Port` is intrinsically linked to its owner process.
    2.  Immediately gets the OS PID of the Python process via `Port.info(port, :os_pid)`.
    3.  Calls the `ProcessRegistry` to **persistently register** its own PID and the external OS PID.
    4.  Starts the actual gRPC connection logic (which could be another process or task supervised by the Guardian).
*   **`terminate/2` (The Golden Rule)**:
    1.  This callback is the **guaranteed cleanup hook for graceful shutdowns**.
    2.  It implements a **graceful-then-forceful** shutdown sequence:
        *   Sends `SIGTERM` to the Python process PID.
        *   Waits for a short period (e.g., 2 seconds) for the process to exit.
        *   If the process is still alive, it sends `SIGKILL` (`kill -9`) to guarantee its termination.
    3.  Calls the `ProcessRegistry` to **unregister** its external OS PID.

This Guardian is then started by the `WorkerSupervisor` with a `restart: :permanent` strategy. If the Guardian crashes, the supervisor restarts it, which triggers its `terminate` (on the old instance) and `init` (on the new one), cleanly replacing the Python process.

#### **Component 2: The Persistent PID Registry (`Snakepit.ProcessRegistry`)**

This is the "transaction persisted" component you described. It acts as the ground truth for which Python processes *should* be alive.

**Architecture:**

*   A GenServer that manages an in-memory ETS table for fast lookups.
*   It also manages a simple, durable **on-disk PID file** (e.g., `/tmp/snakepit_pids.log`). This file is just a text file with one OS PID per line.

**Responsibilities:**

*   **`register_pid(guardian_pid, os_pid)`**:
    1.  Adds the mapping `{os_pid, guardian_pid}` to its ETS table.
    2.  **Appends the `os_pid` to the on-disk PID file.** This is the crucial persistence step.
*   **`unregister_pid(os_pid)`**:
    1.  Removes the mapping from its ETS table.
    2.  **Rewrites the on-disk PID file** to remove the `os_pid`. This is an atomic operation (write to a new file, then rename).
*   **`get_all_pids()`**: Reads all PIDs from the on-disk file. This is for the Reaper and Enforcer.

#### **Component 3: The Startup Reaper**

This component solves the "crashed VM" problem. It's a simple process (can be a GenServer or even just a `Task` in the supervision tree) that runs **once** when the main application starts.

**Responsibilities:**

1.  It is placed **first** in the main supervisor's child list, ensuring it runs before any new workers are started.
2.  In its `init/1` or `start_link/1`:
    *   It calls `ProcessRegistry.get_all_pids()` to read the PID file from the *previous run*.
    *   It iterates through this list of PIDs.
    *   For each PID, it issues a `kill -9 <pid>`. It doesn't matter if the process exists or not; this command will either kill the orphan or fail silently.
    *   After killing all stale PIDs, it clears the PID file, preparing it for the new run.
3.  It then terminates successfully, allowing the rest of the application to boot into a clean state.

#### **Component 4: The Shutdown Enforcer**

This is the final safety net. It lives in the main `Snakepit.Application` module.

**Responsibilities:**

*   Implement the `terminate/2` callback in `lib/snakepit/application.ex`. This callback is invoked by OTP during a graceful application shutdown (e.g., `Application.stop(:snakepit)`).
*   Inside `terminate/2`:
    1.  It calls `ProcessRegistry.get_all_pids()` to get a final list of all known external processes.
    2.  It brutally iterates and sends `SIGKILL` to every single one.
    3.  This handles any race conditions or failures in the individual Guardian `terminate` callbacks. It's the "scorched earth" final guarantee.

### **3. Why This Architecture is Robust**

Let's revisit the failure modes:

1.  **Normal Shutdown**: The application supervisor tells the Worker Supervisor to shut down. It terminates each Guardian. Each Guardian's `terminate/2` callback runs, gracefully killing its Python process and unregistering the PID. The system shuts down cleanly.
2.  **Guardian Crash**: The supervisor immediately restarts the Guardian. The new Guardian's `init` starts a new Python process and registers it. The old (now-dead) Guardian is gone, but its corresponding PID is still in the registry. This is where a *Periodic Janitor* (an enhancement) would come in handy to cross-reference the ETS table with `Process.alive?` on the guardian PIDs.
3.  **BEAM VM Crash (`kill -9`)**: The Guardians never run `terminate`. The Python processes are orphaned. The on-disk PID file remains untouched, containing the PIDs of the orphans. **On the next application start, the Startup Reaper reads this file and immediately kills all the orphaned PIDs before any new workers are created.** The problem is solved.
4.  **Deploy Issues**: Similar to the VM crash, if an old instance is killed uncleanly, the Startup Reaper of the new instance will clean up any orphans.

This architecture provides multiple layers of defense, making it extremely difficult for a Python process to become orphaned. It combines OTP's supervision strengths with explicit, durable OS-level process management.
